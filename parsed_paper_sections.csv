label,content,prov,level
section_header,BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation,"[{'page_no': 1, 'bbox': {'l': 99.64, 't': 717.956, 'r': 512.36, 'b': 683.7, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 82.0]}]",1.0
text,"Rocktim Jyoti Das ∗ 1 , Harsh Singh ∗ 1 , Diana Turmakhan † 1 , Muhammad Abdullah Sohail † 1 , Mingfei Han 1 , Preslav Nakov 1 , Fabio Pizzati 1 , Ivan Laptev 1","[{'page_no': 1, 'bbox': {'l': 66.617, 't': 665.053, 'r': 541.548, 'b': 640.062, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 160.0]}]",
text,"Abstract -Scaling data and models has played a pivotal role in the remarkable progress of computer vision and language. Inspired by these domains, recent efforts in robotics have similarly focused on scaling both data and model size to develop more generalizable and robust policies. However, unlike vision and language, robotics lacks access to internet-scale demonstrations across diverse robotic tasks and environments. As a result, the scale of existing datasets typically suffers from the need for manual data collection and curation. To address this problem, here we propose BLAZER, a framework that learns manipulation policies from automatically generated training data . We build on the zero-shot capabilities of LLM planners and automatically generate demonstrations for diverse manipulation tasks in simulation. Successful examples are then used to finetune an LLM and to improve its planning capabilities without human supervision. Notably, while BLAZER training requires access to the simulator's state, we demonstrate direct transfer of acquired skills to sensor-based manipulation. Through extensive experiments, we show BLAZER to significantly improve zero-shot manipulation in both simulated and real environments. Moreover, BLAZER improves on tasks outside of its training pool and enables downscaling of LLM models. Our code and data will be made publicly available on the project page [1].","[{'page_no': 1, 'bbox': {'l': 54.0, 't': 616.598, 'r': 298.802, 'b': 379.396, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1409.0]}]",
section_header,I. INTRODUCTION,"[{'page_no': 1, 'bbox': {'l': 137.526, 't': 369.725, 'r': 215.282, 'b': 361.173, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 15.0]}]",1.0
text,'A teacher is one who makes himself progressively unnecessary. ',"[{'page_no': 1, 'bbox': {'l': 154.951, 't': 354.918, 'r': 298.801, 'b': 334.375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 64.0]}]",
text,- Thomas Carruthers,"[{'page_no': 1, 'bbox': {'l': 207.712, 't': 330.829, 'r': 298.8, 'b': 322.277, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 19.0]}]",
text,"Learning-based methods are attracting increasingly growing attention in robotics. In particular, extending large language models (LLMs) and vision-language models (VLMs) to robotic tasks promise to empower resulting policies with strong reasoning capabilities and generalization to diverse tasks and environments [2], [3], [4], [5]. However, training generic robotic policies requires large-scale data in the form of paired actions and observations. To address this challenge, recent efforts attempt to collect real-robot demonstrations [6], adopt human videos [7], and leverage simulated environments [8], [9], [10]. Manual collection of robot demonstrations, however, is slow and expensive, while human videos are missing low-level control data and face an embodiment gap between robots and humans. Hence, scaling robotic demonstrations remains a major bottleneck.","[{'page_no': 1, 'bbox': {'l': 54.0, 't': 313.004, 'r': 298.801, 'b': 137.08000000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 866.0]}]",
text,"Recent work on language models brought significant progress improving reasoning capabilities of LLMs [12],","[{'page_no': 1, 'bbox': {'l': 54.0, 't': 133.78499999999997, 'r': 298.801, 'b': 113.27800000000002, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 106.0]}]",
footnote,*Co-first author.,"[{'page_no': 1, 'bbox': {'l': 62.468, 't': 101.91599999999994, 'r': 115.517, 'b': 95.07399999999996, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 17.0]}]",
footnote,† Co-second author.,"[{'page_no': 1, 'bbox': {'l': 62.468, 't': 93.20500000000004, 'r': 125.027, 'b': 86.10799999999995, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 19.0]}]",
text,"1 Mohamed bin Zayed University of Artificial Intelligence, UAE","[{'page_no': 1, 'bbox': {'l': 62.468, 't': 85.30200000000002, 'r': 272.997, 'b': 76.75300000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 62.0]}]",
text,Project Page: https://blazer-llm-agent.github.io/,"[{'page_no': 1, 'bbox': {'l': 62.468, 't': 65.66200000000003, 'r': 276.585, 'b': 58.82000000000005, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 49.0]}]",
text,LLM,"[{'page_no': 1, 'bbox': {'l': 363.707, 't': 505.944, 'r': 391.349, 'b': 492.813, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,BLAZER,"[{'page_no': 1, 'bbox': {'l': 478.058, 't': 509.214, 'r': 516.784, 'b': 499.675, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,LLM,"[{'page_no': 1, 'bbox': {'l': 487.38, 't': 496.355, 'r': 507.461, 'b': 486.816, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,Manipulation,"[{'page_no': 1, 'bbox': {'l': 358.581, 't': 443.26, 'r': 396.512, 'b': 437.946, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 12.0]}]",
text,Environment,"[{'page_no': 1, 'bbox': {'l': 359.543, 't': 436.097, 'r': 395.545, 'b': 430.783, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 11.0]}]",
text,Manipulation,"[{'page_no': 1, 'bbox': {'l': 477.512, 't': 444.926, 'r': 515.442, 'b': 439.611, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 12.0]}]",
text,Environment,"[{'page_no': 1, 'bbox': {'l': 478.473, 't': 437.762, 'r': 514.475, 'b': 432.448, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 11.0]}]",
text,Many Failures!,"[{'page_no': 1, 'bbox': {'l': 348.29, 't': 415.914, 'r': 390.369, 'b': 410.6, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 14.0]}]",
text,Performance Boost!,"[{'page_no': 1, 'bbox': {'l': 461.573, 't': 415.824, 'r': 518.561, 'b': 410.51, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 18.0]}]",
text,BLAZER,"[{'page_no': 1, 'bbox': {'l': 526.407, 't': 524.738, 'r': 545.02, 'b': 520.153, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,Training,"[{'page_no': 1, 'bbox': {'l': 525.666, 't': 518.557, 'r': 545.754, 'b': 513.973, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 8.0]}]",
text,Basketball in Hoop,"[{'page_no': 1, 'bbox': {'l': 318.618, 't': 601.617, 'r': 369.517, 'b': 597.493, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 18.0]}]",
text,Meat off The Grill,"[{'page_no': 1, 'bbox': {'l': 374.493, 't': 601.262, 'r': 422.564, 'b': 597.139, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 18.0]}]",
text,Open Bottle,"[{'page_no': 1, 'bbox': {'l': 441.587, 't': 601.396, 'r': 472.691, 'b': 597.272, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 11.0]}]",
text,Close The Jar,"[{'page_no': 1, 'bbox': {'l': 492.254, 't': 601.211, 'r': 529.014, 'b': 597.087, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 13.0]}]",
text,TASKS,"[{'page_no': 1, 'bbox': {'l': 413.827, 't': 617.767, 'r': 438.082, 'b': 610.493, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 5.0]}]",
text,"[13], [14]. One successful line of work in this direction is based on bootstrapping [13], where pretrained LLMs are first used to generate high-quality rationales and are then finetuned to yield improved performance while using generated rationales as training examples. Bootstrapping and self-improvement have been shown to be particularly effective for problems that require non-trivial solutions and offer simple verification, as for the case of many problems in mathematics and common sense reasoning [15]. We note that robotic manipulation tasks frequently belong to such class of problems since their success can often be certified by merely verifying the end states of manipulated objects.","[{'page_no': 1, 'bbox': {'l': 313.2, 't': 260.424, 'r': 558.001, 'b': 120.36500000000001, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 696.0]}]",
text,"Inspired by the idea of bootstrapping, we propose BLAZER, a framework that bootstraps LLM-based manipulation agents using automatically generated and verified demonstrations. Given language-defined tasks, such as 'stack blocks' or 'open wine bottle' , we follow previous","[{'page_no': 1, 'bbox': {'l': 313.2, 't': 114.77999999999997, 'r': 558.001, 'b': 58.40700000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 270.0]}]",
text,"work [11], [16] and use general-purpose LLM with strong reasoning and coding capabilities to generate executable manipulation plans. Next, we execute such plans in a simulator and evaluate their success. Successful plans for diverse tasks form a training set for supervised finetuning (SFT) of a smaller-scale LLM, which learns to improve manipulation abilities using no human supervision, see Fig. 1.","[{'page_no': 2, 'bbox': {'l': 54.0, 't': 736.449, 'r': 298.801, 'b': 656.166, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 401.0]}]",
text,"The BLAZER training uses privileged information in the form of object locations, orientations, and dimensions provided by the simulator. To deploy BLAZER in the real world, we design a vision pipeline using Molmo [17] and M2T2 [18] for object state estimation. Notably, our LLaMA8B model trained with BLAZER significantly outperforms its initial and larger teacher model LLaMA-70B used to generate training demonstrations. Moreover, despite being trained in simulation, we demonstrate off-the-shelf transfer of BLAZER to real-world manipulation tasks where LLaMA8B ( 47 . 8% ) significantly outperforms the success rate of LLaMA-70B ( 33 . 3% ). We perform extensive experimental evaluation and demonstrate consistent improvements of BLAZER in both the real world and in simulation for a variety of tasks. BLAZER outperforms state-of-the-art zeroshot MALMM [16], it generalizes to tasks unseen during training and requires no manual demonstrations at any stage of the learning process.","[{'page_no': 2, 'bbox': {'l': 54.0, 't': 652.691, 'r': 298.802, 'b': 440.901, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 985.0]}]",
text,"In summary, our work makes the following contributions:","[{'page_no': 2, 'bbox': {'l': 63.963, 't': 437.426, 'r': 298.801, 'b': 428.874, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 55.0]}]",
list_item,"We introduce BLAZER, the framework that bootstraps a generic LLM and enables self-improvement of zeroshot manipulation agents using automatically generated training demonstrations.","[{'page_no': 2, 'bbox': {'l': 64.862, 't': 423.327, 'r': 298.801, 'b': 378.91, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 182.0]}]",
list_item,"Through extensive experiments and ablations, we demonstrate BLAZER to result in significant improvements for a range of manipulation tasks.","[{'page_no': 2, 'bbox': {'l': 64.862, 't': 375.507, 'r': 298.801, 'b': 343.044, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 141.0]}]",
list_item,We further demonstrate off-the-shelf transfer of simulator-trained BLAZER to real-world manipulation tasks while using no manual demonstrations in any part of the training process.,"[{'page_no': 2, 'bbox': {'l': 64.862, 't': 339.641, 'r': 298.801, 'b': 295.224, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 182.0]}]",
section_header,II. RELATED WORK,"[{'page_no': 2, 'bbox': {'l': 133.452, 't': 284.487, 'r': 219.354, 'b': 275.93499999999995, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 16.0]}]",1.0
text,"We discus related work on policy learning, self-improving models and scaling training data for robotics below.","[{'page_no': 2, 'bbox': {'l': 54.0, 't': 268.96399999999994, 'r': 298.801, 'b': 248.457, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 110.0]}]",
section_header,A. Language and Vision Foundational Models for Robotics,"[{'page_no': 2, 'bbox': {'l': 54.0, 't': 237.899, 'r': 296.828, 'b': 229.31100000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 55.0]}]",1.0
text,"Visumotor policies [19], [20], [21] trained on manually collected demonstrations have shown great success in robotic manipulation. However, such models trained from scratch lack generalization to new tasks and environments. The incorporation of vision-language models (VLMs) into end-toend robotic control, in the form of Vision-Language Action Models [22], [2], [3], [4], has enhanced generalization and enabled emergent semantic reasoning. The success of these methods, however, still relies on the collection of large-scale human demonstrations, which is expensive. ManipLLM [23] explores VLM finetuning with chain-of-thought reasoning for robotic manipulation tasks. This approach, however, requires test-time adaptation to overcome sim-to-real gap. In contrast, our framework enables self-improvement using no human supervision. Moreover, our LLM agents, combined with our vision pipeline, directly transfer to real-world tasks without additional training.","[{'page_no': 2, 'bbox': {'l': 54.0, 't': 222.37599999999998, 'r': 298.801, 'b': 58.40700000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 814.0]}, {'page_no': 2, 'bbox': {'l': 313.2, 't': 736.448, 'r': 558.001, 'b': 703.986, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [815.0, 961.0]}]",
text,"To overcome the need of large-scale training data and to facilitate generalization to new tasks, recent work explores LLMs and VLMs for zero-shot robotic manipulation. Code as Policies [11] and MALMM [16] deploy LLM for writing robot policy code, while Voxposer [24] and GenSim [8], [9] couple code generation capabiltities of LLM with multimodal reasoning capabilities of VLM to generate 3D value map for trajectory estimation and to automatically create simulator tasks, respectively. AHA [25] demonstrates that VLMs can be used for detecting and adapting to failures. Our approach is closely related to MALMM [16] and Code as Policies [11]. While these methods remain dependent on careful prompt engineering and require much computation at test time, our approach enables self-improvement using automatically generated training data and can run using relatively small and efficient LLMs at inference time.","[{'page_no': 2, 'bbox': {'l': 313.2, 't': 700.422, 'r': 558.001, 'b': 512.542, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 908.0]}]",
section_header,B. Self-improving models,"[{'page_no': 2, 'bbox': {'l': 313.2, 't': 501.225, 'r': 417.14, 'b': 492.637, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 24.0]}]",1.0
text,"Recent work improves the reasoning capabilities of LLMs by generating few-shot rationales [26], [12], [27], bootstrapping [13], and reinforcement learning [28], [29], [15]. Among these approaches, self-training [13] stands out as a particularly scalable and successful strategy. In this approach, general-purpose LLMs are first used to generate high-quality rationales that are later deployed to train either improved versions of original models or smaller models [30].","[{'page_no': 2, 'bbox': {'l': 313.2, 't': 485.39, 'r': 558.001, 'b': 393.152, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 469.0]}]",
text,"The idea of self-improvement has also been explored in robotics e.g., to learn low-level visuomotor policies from a large dataset of grasping attempts [31] or from hourslong object poking interactions [32]. More recent methods focus on correcting manipulation failures at test time by reasoning about past experience [33] or detecting misalignment between planned and executed actions [34]. Another work, ReFineVLA [35], augments manipulation datasets with action rationales using Gemini [36], thereby enabling VLAs to reason about their actions. Related to our approach, SC-VLA [37] enables self-correction of VLA models from successful task executions. This parallel work, however, is mostly focused on low-level pushing and pulling actions, and requires explicit failure detection. In contrast, our bootstrapping approach only relies on success verification at the task level and addresses a variety of complex tasks that require high-level reasoning.","[{'page_no': 2, 'bbox': {'l': 313.2, 't': 389.587, 'r': 558.001, 'b': 189.75299999999993, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 954.0]}]",
section_header,C. Data Generation for Manipulation,"[{'page_no': 2, 'bbox': {'l': 313.2, 't': 178.43600000000004, 'r': 468.806, 'b': 169.84799999999996, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 35.0]}]",1.0
text,"Many efforts have been dedicated to scale up training data for manipulation tasks. Some works collect real-world grasping data [31], [6] by deploying random trials followed by verification as well as collection of human demonstrations. KALIE [38] curated human-annotated affordance data as an alternative to robot demonstrations, whereas PhysObjects [39] collected an object-centric dataset to train a VLM with the physical concepts of common household objects. Another approach, LLaRA [40], adopted behavior cloning","[{'page_no': 2, 'bbox': {'l': 313.2, 't': 162.60000000000002, 'r': 558.001, 'b': 58.40700000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 516.0]}]",
text,Close Jar,"[{'page_no': 3, 'bbox': {'l': 179.761, 't': 715.846, 'r': 229.155, 'b': 705.367, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 9.0]}]",
text,Data Generation,"[{'page_no': 3, 'bbox': {'l': 163.598, 't': 674.249, 'r': 248.609, 'b': 664.425, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 15.0]}]",
text,ε,"[{'page_no': 3, 'bbox': {'l': 171.024, 't': 699.955, 'r': 178.891, 'b': 682.026, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,τ,"[{'page_no': 3, 'bbox': {'l': 232.935, 't': 690.59, 'r': 237.703, 'b': 680.805, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,τ,"[{'page_no': 3, 'bbox': {'l': 253.743, 't': 637.076, 'r': 257.624, 'b': 629.11, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,τ,"[{'page_no': 3, 'bbox': {'l': 240.773, 't': 723.927, 'r': 251.566, 'b': 701.778, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,ρ,"[{'page_no': 3, 'bbox': {'l': 230.427, 't': 699.799, 'r': 238.18, 'b': 687.349, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,Supervised Finetuning,"[{'page_no': 3, 'bbox': {'l': 324.873, 't': 618.796, 'r': 403.791, 'b': 612.172, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 21.0]}]",
text,T,"[{'page_no': 3, 'bbox': {'l': 104.399, 't': 736.361, 'r': 117.79, 'b': 715.425, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,LLM,"[{'page_no': 3, 'bbox': {'l': 290.649, 't': 599.851, 'r': 308.629, 'b': 591.31, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,ε,"[{'page_no': 3, 'bbox': {'l': 280.783, 't': 724.687, 'r': 286.84, 'b': 710.884, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,ε,"[{'page_no': 3, 'bbox': {'l': 337.447, 't': 690.907, 'r': 341.162, 'b': 682.439, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,∑,"[{'page_no': 3, 'bbox': {'l': 329.48, 't': 694.841, 'r': 337.77, 'b': 686.843, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,τ,"[{'page_no': 3, 'bbox': {'l': 283.413, 't': 661.885, 'r': 286.739, 'b': 655.06, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,ρ,"[{'page_no': 3, 'bbox': {'l': 281.664, 't': 668.309, 'r': 287.072, 'b': 659.624, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,Environment,"[{'page_no': 3, 'bbox': {'l': 289.668, 't': 720.505, 'r': 326.377, 'b': 712.676, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 11.0]}]",
text,Object,"[{'page_no': 3, 'bbox': {'l': 343.342, 't': 695.018, 'r': 359.959, 'b': 688.297, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,States,"[{'page_no': 3, 'bbox': {'l': 344.422, 't': 688.265, 'r': 358.881, 'b': 681.545, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,Commands,"[{'page_no': 3, 'bbox': {'l': 406.121, 't': 673.396, 'r': 437.513, 'b': 665.567, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 8.0]}]",
text,Task Description,"[{'page_no': 3, 'bbox': {'l': 289.165, 't': 664.827, 'r': 336.04, 'b': 656.998, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 16.0]}]",
text,'Close the jar.',"[{'page_no': 3, 'bbox': {'l': 294.978, 't': 652.342, 'r': 329.334, 'b': 645.557, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 16.0]}]",
text,LLM,"[{'page_no': 3, 'bbox': {'l': 367.798, 't': 688.142, 'r': 383.386, 'b': 680.737, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,Boot,"[{'page_no': 3, 'bbox': {'l': 384.084, 't': 683.542, 'r': 395.209, 'b': 676.936, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 4.0]}]",
text,Task Execution,"[{'page_no': 3, 'bbox': {'l': 454.558, 't': 719.901, 'r': 490.124, 'b': 713.263, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 14.0]}]",
text,τ,"[{'page_no': 3, 'bbox': {'l': 421.83, 't': 684.477, 'r': 425.756, 'b': 676.421, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,C,"[{'page_no': 3, 'bbox': {'l': 415.518, 't': 689.44, 'r': 422.077, 'b': 679.179, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,VERIFICATION,"[{'page_no': 3, 'bbox': {'l': 450.593, 't': 652.043, 'r': 486.74, 'b': 645.258, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 12.0]}]",
text,BLAZER,"[{'page_no': 3, 'bbox': {'l': 463.633, 't': 623.253, 'r': 495.72, 'b': 615.349, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,LLM,"[{'page_no': 3, 'bbox': {'l': 471.357, 't': 612.599, 'r': 487.995, 'b': 604.695, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,datasets for instruction tuning tailored for manipulation tasks. This approach relies of external demonstrations and uses manually curated templates to generate question-answer pairs.,"[{'page_no': 3, 'bbox': {'l': 54.0, 't': 505.33, 'r': 298.801, 'b': 460.912, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 183.0]}]",
text,"Another popular direction is to collect demonstrations in simulation. Nevertheless, this approach is still challenging as it requires significant human efforts to create diverse simulation tasks and scenes to allow generalization of the learned policies. To address this issue, recent work [8], [9] has leveraged coding LLMs with multimodal reasoning capabilities to generate complex and realistic simulation tasks without human supervision. DemoGen [41] adapted a single human trajectory to novel object configurations using 3D point cloud editing, yielding synthetic demonstrations that improve manipulation policies. Unlike prior work, our work focuses on scaling up demonstrations for a given set of tasks, while preserving generalization capabilities of LLMs. We propose a framework where we can generate an arbitrary number of data in simulation, benefiting from the reasoning and coding capabilities of LLMs to synthesize verification data with no human intervention. We subsequently use the generated data for the training of our agent tailored for manipulation.","[{'page_no': 3, 'bbox': {'l': 54.0, 't': 456.859, 'r': 298.801, 'b': 233.11400000000003, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1070.0]}]",
section_header,III. METHOD,"[{'page_no': 3, 'bbox': {'l': 147.534, 't': 217.46000000000004, 'r': 205.273, 'b': 208.90800000000002, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 11.0]}]",1.0
text,"Here, we introduce the BLAZER methodology for training specialized LLM agents for robotic manipulation. In short, we aim to finetune an existing lightweight LLM on synthetically-generated data, tuning it for the generation of manipulation-oriented robotics policies. An overview of our method is shown in Fig. 2. This section first introduces the formalization and the problem setup (Section III-A), and we further describe how the BLAZER training works in Section III-B. Since our training procedure only exploits automatic annotations generated by a simulator, we also introduce a vision-based pipeline for the deployment in the real world of the trained LLM agent (Section III-C).","[{'page_no': 3, 'bbox': {'l': 54.0, 't': 198.466, 'r': 298.801, 'b': 58.40700000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 683.0]}]",
section_header,A. Formalization and background,"[{'page_no': 3, 'bbox': {'l': 313.2, 't': 505.508, 'r': 451.59, 'b': 496.92, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 31.0]}]",1.0
text,"We assume a manipulation environment E , where a robotic gripper G interacts with objects to solve a task τ that we can automatically verify in simulation. To achieve this, G needs to manipulate a set of K objects { o 1 , o 2 , ..., o K } in E , using a sequence of I control functions sampled from a primitives set F = { open gripper , close gripper , move gripper } . Primitives in F are combined to define a policy composed of control commands C τ . Formally, this is written as follows:","[{'page_no': 3, 'bbox': {'l': 313.2, 't': 490.613, 'r': 558.005, 'b': 397.297, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 490.0]}]",
formula,,"[{'page_no': 3, 'bbox': {'l': 369.929, 't': 390.581, 'r': 557.997, 'b': 377.91, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 50.0]}]",
text,"The primitives open gripper and close gripper can be executed without additional knowledge of the environment. In contrast, move gripper requires as input the final desired gripper position and orientation. During execution, the gripper joints' positions are computed with inverse kinematics.","[{'page_no': 3, 'bbox': {'l': 313.2, 't': 369.951, 'r': 558.003, 'b': 313.399, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 292.0]}]",
text,"Following the open-loop single-agent setup introduced in MALMM [16], C τ can be obtained by prompting largescale LLMs such as GPT-4 [42]. Logically, in this case the LLM must also generate all the code necessary for a correct execution. To achieve this, we define the current state of the environment as Σ E = { o i } K i =1 , where each o i = [ ξ i , ϕ i , l i , w i , h i ] , ∀ i ∈ [1 , K ] encodes the 3D center position ξ i of the object o i , the orientation on the z-axis ϕ i , and the metric dimensions l (length), w (width) and h (height). We also define ρ τ as a textual description of τ , to enable LLM prompting. We then derive C τ by prompting the LLM with ρ τ and a textual representation of Σ E . For more details, refer to [16]. In short, the control commands generation is expressed as follows:","[{'page_no': 3, 'bbox': {'l': 313.2, 't': 310.055, 'r': 558.005, 'b': 146.08500000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 810.0]}]",
formula,,"[{'page_no': 3, 'bbox': {'l': 394.505, 't': 136.53700000000003, 'r': 558.0, 'b': 126.90800000000002, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 29.0]}]",
section_header,B. Bootstrapping manipulation agents with simulation,"[{'page_no': 3, 'bbox': {'l': 313.2, 't': 118.03800000000001, 'r': 535.764, 'b': 109.45000000000005, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 52.0]}]",1.0
text,"In many scenarios, naively prompting an LLM for C τ as in Eq. 2 results in planning or coding errors [16]. While MALMM mitigates the problem with an expensive multiagent pipeline, our goal is instead to train a single LLM","[{'page_no': 3, 'bbox': {'l': 313.2, 't': 103.14300000000003, 'r': 558.001, 'b': 58.40700000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 221.0]}]",
text,"agent LLMBLAZER, with specific capabilities tuned ad hoc on manipulation tasks.","[{'page_no': 4, 'bbox': {'l': 54.0, 't': 736.449, 'r': 298.802, 'b': 715.941, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 79.0]}]",
text,"Our idea is to obtain LLMBLAZER by finetuning a pretrained lightweight language model without requiring human supervision, as we show in Figure 2. To do so, we use synthetic examples tailored for C τ generation, obtained within the interactions of an LLM with a manipulationoriented simulator such as CoppeliaSim. We first define a set of T representative tasks T = { τ 1 , τ 2 , ..., τ T } that we use for data generation. Each task is processed by a language model LLMboot to generate C τ as described in Section III-A. Importantly, since we operate in a simulated environment, we can automatically verify if C τ successfully solves τ , thanks to the automatic verification criteria in manipulation-oriented simulators. Hence, for given commands C τ and a task τ , we associate a verification operator V defined as follows:","[{'page_no': 4, 'bbox': {'l': 54.0, 't': 712.564, 'r': 298.805, 'b': 548.594, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 825.0]}]",
formula,,"[{'page_no': 4, 'bbox': {'l': 62.806, 't': 541.891, 'r': 298.8, 'b': 503.142, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 77.0]}]",
text,"Now, for each task τ we automatically collect a dataset D τ of N successful solutions provided by LLMboot. While this would be extremely challenging with real demonstrations, in a simulated environment, we can easily randomize the initial state Σ E to obtain an arbitrary number of object configurations for each task in T , and automatically verify the correctness of proposed commands. We illustrate this process in Fig. 2. For any τ ∈ T ,","[{'page_no': 4, 'bbox': {'l': 54.0, 't': 500.057, 'r': 298.801, 'b': 407.501, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 441.0]}]",
formula,,"[{'page_no': 4, 'bbox': {'l': 107.461, 't': 400.795, 'r': 298.8, 'b': 372.92, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 89.0]}]",
text,"In Eq. (4), we call Σ i E a random state configuration sampled in the simulated environment. Note that it is impossible to only generate Σ i E such that C τ is successful; hence, part of the generated commands resulting in unacceptable C τ will be discarded by our simulator-in-the-loop verification strategy. Finally, we train LLMBLAZER using Supervised Finetuning (SFT) on a dataset D BLAZER resulting from the aggregation of all D τ :","[{'page_no': 4, 'bbox': {'l': 54.0, 't': 366.564, 'r': 298.805, 'b': 271.674, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 437.0]}]",
formula,,"[{'page_no': 4, 'bbox': {'l': 101.487, 't': 264.828, 'r': 298.8, 'b': 238.77600000000007, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 74.0]}]",
text,"Note that our training uses a generic target LLM, and hence we do not impose the finetuning of LLMboot as in self-refinement strategies [13]. This allows to benefit from larger LLMs for data generation, easing the generation of successful commands. Ultimately, our simulated training with verification allows filtering of any wrong solution, automatically curating a dataset for SFT with no human intervention .","[{'page_no': 4, 'bbox': {'l': 54.0, 't': 232.062, 'r': 298.801, 'b': 139.82400000000007, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 411.0]}]",
section_header,C. Vision pipeline,"[{'page_no': 4, 'bbox': {'l': 54.0, 't': 130.02700000000004, 'r': 128.431, 'b': 121.43899999999996, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 18.0]}]",1.0
text,"While in simulation we can extract the ground truth space Σ E , in a real-world deployment, we must instead estimate its approximation ˜ Σ E purely from visual data. Hence, to enable the deployment of LLMBLAZER outside the simulator, we introduce a vision pipeline, based on existing foundation models and no training . This allows us to mitigate the distribution shift that would occur if we trained perception components directly on a simulated environment [43].","[{'page_no': 4, 'bbox': {'l': 54.0, 't': 114.77999999999997, 'r': 298.802, 'b': 58.40700000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 294.0]}, {'page_no': 4, 'bbox': {'l': 313.2, 't': 736.627, 'r': 558.001, 'b': 703.986, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [295.0, 464.0]}]",
text,"We assume a multi-view setup that can use any number of calibrated RGB-D cameras. As a preliminary step, we prompt GPT-4o for a list of elements present in the scene, conditioned on a given task τ . Then, in all views, we first prompt Molmo [17] to extract the 2D coordinates of the center of each object o i in a task τ . Employing Molmo for prompt interpretation enables contextual and spatial reasoning, going beyond simple semantics [17]. Next, we use the 2D center coordinates as a prompt for Segment Anything [44], obtaining a segmentation mask of the object in 2D for that view. By combining the segmentation mask with the RGB-D data, we derive a 3D bounding box for the object, which provides its estimated dimensions. Finally, we use M2T2 [18] to predict the object's 3D center position and grasping orientation. We aggregate the outputs from multiple views with a simple median filter to remove outliers, obtaining the final ( ˜ l, ˜ w, ˜ h ) from the bounding box and ˜ ξ i , ˜ ϕ i from M2T2. We then construct ˜ o = { ˜ ξ i , ˜ ϕ i , ˜ l, ˜ w, ˜ h } . Repeating this for each object in the space gives ˜ Σ E = { ˜ o i } K i =1 . We use ˜ Σ E as input to LLMBLAZER in real-world deployment.","[{'page_no': 4, 'bbox': {'l': 313.2, 't': 700.542, 'r': 558.004, 'b': 464.842, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1201.0]}]",
section_header,IV. EXPERIMENTS,"[{'page_no': 4, 'bbox': {'l': 395.984, 't': 454.371, 'r': 475.225, 'b': 445.819, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 15.0]}]",1.0
text,Our experiments are designed to assess the LLM agent trained using our BLAZER framework against existing vision and language foundation model-based baselines.,"[{'page_no': 4, 'bbox': {'l': 313.2, 't': 439.035, 'r': 558.001, 'b': 406.573, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 158.0]}]",
section_header,A. Experimental details,"[{'page_no': 4, 'bbox': {'l': 313.2, 't': 396.281, 'r': 410.495, 'b': 387.693, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 23.0]}]",1.0
text,"Tasks and environment. For comparison with baselines, we follow the MALMM setup and evaluate on nine simulated tasks from RLBench [45], illustrated in Fig. 3: Basketball in Hoop (BH), Close Jar (CJ), Empty Container (EC), Insert in Peg (IP), Meat off the Grill (MG), Open Bottle (OB), Put Block (PB), Rubbish in Bin (RB), and Stack Blocks (SB). Detailed task descriptions and success conditions are provided in Appendix-B. We train BLAZER by setting these tasks as T . During testing, we randomize each test episode configuration, preventing overlaps with those used in training. We use CoppeliaSim and interface it with PyRep. In the real-world setup, we test generalization on 12 tasks on a tabletop using a 7-DOF Franka Emika Panda Research 3 robot equipped with a parallel jaw gripper. We use three Intel RealSense D435i RGB-D cameras to capture the frontal, right, and left views and the panda-py [46] library to control the robot arm.","[{'page_no': 4, 'bbox': {'l': 313.2, 't': 381.255, 'r': 558.001, 'b': 181.03200000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 940.0]}]",
text,"Implementation Details. For our experiments, we choose LLaMA-3.1-8B (LLaMA-8B) as LLMBLAZER, and LLaMA3.3-70B (LLaMA-70B) as LLMboot. The prompt for data bootstrapping is adopted from Single Agent (SA) setup of MALMM [16], and is included in Appendix-A. Both the LLMs, LLMBLAZER and LLMboot generate 3D waypoints for the gripper, while trajectories are computed and executed using a motion planner, which is a common approach in RLBench [45]. During SFT, the models are trained with a prompt completion loss, using N = 2000 examples per task.","[{'page_no': 4, 'bbox': {'l': 313.2, 't': 174.94299999999998, 'r': 558.004, 'b': 58.40700000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 542.0]}]",
text,Basketball,"[{'page_no': 5, 'bbox': {'l': 75.566, 't': 740.157, 'r': 109.136, 'b': 733.287, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 10.0]}]",
text,in Hoop,"[{'page_no': 5, 'bbox': {'l': 79.005, 't': 731.82, 'r': 105.705, 'b': 724.95, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 7.0]}]",
text,Close,"[{'page_no': 5, 'bbox': {'l': 138.059, 't': 740.184, 'r': 156.215, 'b': 733.314, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 5.0]}]",
text,Jar,"[{'page_no': 5, 'bbox': {'l': 141.872, 't': 731.82, 'r': 152.217, 'b': 724.95, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,Empty,"[{'page_no': 5, 'bbox': {'l': 191.737, 't': 741.794, 'r': 212.101, 'b': 734.924, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 5.0]}]",
text,Container,"[{'page_no': 5, 'bbox': {'l': 185.663, 't': 731.82, 'r': 217.99, 'b': 724.95, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 9.0]}]",
text,Insert,"[{'page_no': 5, 'bbox': {'l': 247.331, 't': 740.157, 'r': 265.925, 'b': 733.287, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,in Peg,"[{'page_no': 5, 'bbox': {'l': 246.486, 't': 731.82, 'r': 266.913, 'b': 724.95, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,Meat,"[{'page_no': 5, 'bbox': {'l': 303.221, 't': 740.332, 'r': 319.6, 'b': 733.462, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 4.0]}]",
text,off Grill,"[{'page_no': 5, 'bbox': {'l': 298.196, 't': 731.82, 'r': 324.76, 'b': 724.95, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 9.0]}]",
text,Open,"[{'page_no': 5, 'bbox': {'l': 357.632, 't': 741.91, 'r': 374.895, 'b': 735.04, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 4.0]}]",
text,Bottle,"[{'page_no': 5, 'bbox': {'l': 356.744, 't': 731.82, 'r': 375.785, 'b': 724.95, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,Put,"[{'page_no': 5, 'bbox': {'l': 415.439, 't': 740.332, 'r': 426.509, 'b': 733.462, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,Block,"[{'page_no': 5, 'bbox': {'l': 411.984, 't': 731.82, 'r': 429.973, 'b': 724.95, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 5.0]}]",
text,Rubbish,"[{'page_no': 5, 'bbox': {'l': 462.764, 't': 740.157, 'r': 488.89, 'b': 733.287, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 7.0]}]",
text,in Bin,"[{'page_no': 5, 'bbox': {'l': 465.797, 't': 731.82, 'r': 485.858, 'b': 724.95, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,Stack,"[{'page_no': 5, 'bbox': {'l': 521.99, 't': 740.408, 'r': 539.094, 'b': 733.538, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 5.0]}]",
text,Blocks,"[{'page_no': 5, 'bbox': {'l': 520.065, 't': 731.82, 'r': 541.154, 'b': 724.95, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,Start,"[{'page_no': 5, 'bbox': {'l': 55.061, 't': 708.191, 'r': 63.649, 'b': 688.814, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 5.0]}]",
text,End,"[{'page_no': 5, 'bbox': {'l': 55.151, 't': 651.726, 'r': 63.739, 'b': 635.676, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,"We train for 5 epochs with an effective batch size of 24. We adopted parameter-efficient finetuning via LoRA with a rank of 64 and a scaling factor ( α ) of 16. The learning rate is 2e-5, with a cosine learning rate scheduler applied during training.","[{'page_no': 5, 'bbox': {'l': 54.0, 't': 375.927, 'r': 298.801, 'b': 319.555, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 250.0]}]",
text,"Baselines. We compare BLAZER to other methods using no manual supervision: CAP [11], VoxPoser [24] and MALMM [16]. All baseline results are reported following MALMM [16] and use GPT-4 Turbo [42] as the underlying LLM model. Furthermore, we also test two zero-shot LLM baselines, which include LLaMA-70B and LLaMA-8B. To do so, we query each LLM with the prompt used for D BLAZER generation, and directly apply the obtained C τ as policy. Note that in BLAZER we use LLaMA-70B as LLMboot and LLaMA-8B as LLMBLAZER, so those baselines serve as comparison with respect to the base version of the LLMs used in our framework.","[{'page_no': 5, 'bbox': {'l': 54.0, 't': 314.344, 'r': 298.801, 'b': 173.89699999999993, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 619.0]}]",
section_header,B. Performance analysis,"[{'page_no': 5, 'bbox': {'l': 54.0, 't': 165.42499999999995, 'r': 154.921, 'b': 156.837, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 23.0]}]",1.0
text,"Comparison to baselines In Table I, we present the performance of our LLMBLAZER model on 100 episodes for each task in T , along with results of baselines introduced in Sec. IV-A. Here we test all models assuming the knowledge of ground truth states Σ E provided by the simulator.","[{'page_no': 5, 'bbox': {'l': 54.0, 't': 150.78700000000003, 'r': 298.801, 'b': 93.36599999999999, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 280.0]}]",
text,Our LLaMA-8B model trained with BLAZER outperforms all baselines on average ( 83.2% success). This shows that the bootstrapping of reasoning data for manipulation tasks,"[{'page_no': 5, 'bbox': {'l': 54.0, 't': 91.048, 'r': 298.801, 'b': 58.40700000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 168.0]}]",
text,BH,"[{'page_no': 5, 'bbox': {'l': 350.919, 't': 248.19000000000005, 'r': 357.783, 'b': 241.548, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,CJ,"[{'page_no': 5, 'bbox': {'l': 371.17, 't': 248.19000000000005, 'r': 378.034, 'b': 241.548, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,EC,"[{'page_no': 5, 'bbox': {'l': 391.421, 't': 248.19000000000005, 'r': 398.284, 'b': 241.548, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,IP,"[{'page_no': 5, 'bbox': {'l': 411.671, 't': 248.19000000000005, 'r': 418.535, 'b': 241.548, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,MG,"[{'page_no': 5, 'bbox': {'l': 431.922, 't': 248.19000000000005, 'r': 438.786, 'b': 241.548, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,OB,"[{'page_no': 5, 'bbox': {'l': 452.173, 't': 248.19000000000005, 'r': 459.037, 'b': 241.548, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,PB,"[{'page_no': 5, 'bbox': {'l': 472.423, 't': 248.19000000000005, 'r': 479.287, 'b': 241.548, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,RB,"[{'page_no': 5, 'bbox': {'l': 492.674, 't': 248.19000000000005, 'r': 499.538, 'b': 241.548, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,SB,"[{'page_no': 5, 'bbox': {'l': 512.925, 't': 248.19000000000005, 'r': 519.789, 'b': 241.548, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,AVG,"[{'page_no': 5, 'bbox': {'l': 531.458, 't': 248.19000000000005, 'r': 541.754, 'b': 241.548, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,Tasks,"[{'page_no': 5, 'bbox': {'l': 434.751, 't': 241.486, 'r': 456.201, 'b': 233.18399999999997, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 5.0]}]",
text,0,"[{'page_no': 5, 'bbox': {'l': 330.959, 't': 252.86599999999999, 'r': 334.391, 'b': 246.22500000000002, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,20,"[{'page_no': 5, 'bbox': {'l': 327.524, 't': 274.06999999999994, 'r': 334.388, 'b': 267.429, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,40,"[{'page_no': 5, 'bbox': {'l': 327.524, 't': 295.274, 'r': 334.388, 'b': 288.632, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,60,"[{'page_no': 5, 'bbox': {'l': 327.524, 't': 316.478, 'r': 334.388, 'b': 309.836, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,80,"[{'page_no': 5, 'bbox': {'l': 327.524, 't': 337.682, 'r': 334.388, 'b': 331.04, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,100,"[{'page_no': 5, 'bbox': {'l': 324.089, 't': 358.886, 'r': 334.385, 'b': 352.244, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,Success Rate (%),"[{'page_no': 5, 'bbox': {'l': 314.562, 't': 337.062, 'r': 322.864, 'b': 268.423, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 16.0]}]",
text,32,"[{'page_no': 5, 'bbox': {'l': 345.585, 't': 291.612, 'r': 352.226, 'b': 284.748, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,0,"[{'page_no': 5, 'bbox': {'l': 365.835, 't': 254.25299999999993, 'r': 372.477, 'b': 250.82100000000003, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,0,"[{'page_no': 5, 'bbox': {'l': 386.086, 't': 254.25299999999993, 'r': 392.728, 'b': 250.82100000000003, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,4,"[{'page_no': 5, 'bbox': {'l': 406.337, 't': 258.494, 'r': 412.978, 'b': 255.062, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,16,"[{'page_no': 5, 'bbox': {'l': 426.588, 't': 274.648, 'r': 433.229, 'b': 267.784, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,32,"[{'page_no': 5, 'bbox': {'l': 446.838, 't': 291.612, 'r': 453.48, 'b': 284.748, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,72,"[{'page_no': 5, 'bbox': {'l': 467.089, 't': 334.019, 'r': 473.731, 'b': 327.155, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,16,"[{'page_no': 5, 'bbox': {'l': 487.34, 't': 274.648, 'r': 493.981, 'b': 267.784, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,0,"[{'page_no': 5, 'bbox': {'l': 507.59, 't': 254.25299999999993, 'r': 514.232, 'b': 250.82100000000003, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,76,"[{'page_no': 5, 'bbox': {'l': 350.647, 't': 338.26, 'r': 357.289, 'b': 331.396, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,76,"[{'page_no': 5, 'bbox': {'l': 370.898, 't': 338.26, 'r': 377.54, 'b': 331.396, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,8,"[{'page_no': 5, 'bbox': {'l': 391.149, 't': 262.735, 'r': 397.79, 'b': 259.303, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,12,"[{'page_no': 5, 'bbox': {'l': 411.4, 't': 270.408, 'r': 418.041, 'b': 263.544, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,40,"[{'page_no': 5, 'bbox': {'l': 431.65, 't': 300.093, 'r': 438.292, 'b': 293.229, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,40,"[{'page_no': 5, 'bbox': {'l': 451.901, 't': 300.093, 'r': 458.543, 'b': 293.229, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,76,"[{'page_no': 5, 'bbox': {'l': 472.152, 't': 338.26, 'r': 478.793, 'b': 331.396, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,56,"[{'page_no': 5, 'bbox': {'l': 492.402, 't': 317.056, 'r': 499.044, 'b': 310.192, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,16,"[{'page_no': 5, 'bbox': {'l': 512.653, 't': 274.648, 'r': 519.295, 'b': 267.784, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,88,"[{'page_no': 5, 'bbox': {'l': 355.71, 't': 350.983, 'r': 362.352, 'b': 344.119, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,92,"[{'page_no': 5, 'bbox': {'l': 375.961, 't': 355.223, 'r': 382.602, 'b': 348.359, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,12,"[{'page_no': 5, 'bbox': {'l': 396.212, 't': 270.408, 'r': 402.853, 'b': 263.544, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,16,"[{'page_no': 5, 'bbox': {'l': 416.462, 't': 274.648, 'r': 423.104, 'b': 267.784, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,52,"[{'page_no': 5, 'bbox': {'l': 436.713, 't': 312.816, 'r': 443.355, 'b': 305.952, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,60,"[{'page_no': 5, 'bbox': {'l': 456.964, 't': 321.297, 'r': 463.605, 'b': 314.433, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,100,"[{'page_no': 5, 'bbox': {'l': 477.214, 't': 367.137, 'r': 483.856, 'b': 356.841, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,84,"[{'page_no': 5, 'bbox': {'l': 497.465, 't': 346.742, 'r': 504.107, 'b': 339.878, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,24,"[{'page_no': 5, 'bbox': {'l': 517.716, 't': 283.13, 'r': 524.357, 'b': 276.2660000000001, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,19,"[{'page_no': 5, 'bbox': {'l': 527.841, 't': 277.82899999999995, 'r': 534.483, 'b': 270.9649999999999, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,44,"[{'page_no': 5, 'bbox': {'l': 532.904, 't': 304.334, 'r': 539.545, 'b': 297.47, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,59,"[{'page_no': 5, 'bbox': {'l': 537.966, 't': 320.237, 'r': 544.608, 'b': 313.373, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,LLaMA 8B,"[{'page_no': 5, 'bbox': {'l': 356.061, 't': 377.32, 'r': 386.949, 'b': 369.848, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 8.0]}]",
text,LLaMA 70B,"[{'page_no': 5, 'bbox': {'l': 417.756, 't': 377.32, 'r': 452.505, 'b': 369.848, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 9.0]}]",
text,LLaMA 8B w/ BLAZER,"[{'page_no': 5, 'bbox': {'l': 483.315, 't': 377.32, 'r': 552.812, 'b': 369.848, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 18.0]}]",
text,"and the training of LLMs leads to stronger agents for robotic manipulation, proving the effectiveness of BLAZER. Importantly, our LLaMA-8B model with BLAZER surpasses considerably ( +6.2% on average) LLaMA-70B, which we used as LLMboot, using only a fraction of the parameters. Moreover, it substantially increases the capabilities of the","[{'page_no': 5, 'bbox': {'l': 313.2, 't': 126.73500000000001, 'r': 558.001, 'b': 58.40700000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 338.0]}]",
text,In-distribution tasks,"[{'page_no': 6, 'bbox': {'l': 110.529, 't': 739.548, 'r': 183.731, 'b': 731.818, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 21.0]}]",
text,Out-of-distribution tasks,"[{'page_no': 6, 'bbox': {'l': 350.084, 't': 739.548, 'r': 439.219, 'b': 731.818, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 25.0]}]",
text,Stack,"[{'page_no': 6, 'bbox': {'l': 81.058, 't': 723.446, 'r': 98.162, 'b': 716.576, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 5.0]}]",
text,Blocks,"[{'page_no': 6, 'bbox': {'l': 79.133, 't': 714.859, 'r': 100.222, 'b': 707.989, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,Put,"[{'page_no': 6, 'bbox': {'l': 138.52, 't': 723.371, 'r': 149.59, 'b': 716.501, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,Block,"[{'page_no': 6, 'bbox': {'l': 135.065, 't': 714.859, 'r': 153.054, 'b': 707.989, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 5.0]}]",
text,Rubbish,"[{'page_no': 6, 'bbox': {'l': 185.514, 't': 723.195, 'r': 211.64, 'b': 716.325, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 7.0]}]",
text,in Bin,"[{'page_no': 6, 'bbox': {'l': 188.546, 't': 714.859, 'r': 208.607, 'b': 707.989, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,Block in,"[{'page_no': 6, 'bbox': {'l': 243.537, 't': 723.371, 'r': 270.516, 'b': 716.501, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 8.0]}]",
text,R/L Basket,"[{'page_no': 6, 'bbox': {'l': 239.218, 't': 714.859, 'r': 274.693, 'b': 707.989, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 10.0]}]",
text,SW/SO Fruit,"[{'page_no': 6, 'bbox': {'l': 290.746, 't': 723.446, 'r': 332.063, 'b': 716.576, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 11.0]}]",
text,in Bowl,"[{'page_no': 6, 'bbox': {'l': 298.784, 't': 714.859, 'r': 324.161, 'b': 707.989, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 7.0]}]",
text,Fruit in,"[{'page_no': 6, 'bbox': {'l': 353.457, 't': 723.371, 'r': 378.395, 'b': 716.501, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 8.0]}]",
text,Cl. Basket,"[{'page_no': 6, 'bbox': {'l': 349.113, 't': 714.859, 'r': 382.595, 'b': 707.989, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 10.0]}]",
text,Cup on,"[{'page_no': 6, 'bbox': {'l': 408.353, 't': 724.949, 'r': 432.399, 'b': 718.079, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,C. Object,"[{'page_no': 6, 'bbox': {'l': 404.631, 't': 714.859, 'r': 435.977, 'b': 707.989, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 9.0]}]",
text,Jar,"[{'page_no': 6, 'bbox': {'l': 469.561, 't': 723.223, 'r': 479.906, 'b': 716.353, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,in Bin,"[{'page_no': 6, 'bbox': {'l': 464.795, 't': 714.859, 'r': 484.856, 'b': 707.989, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,Case on,"[{'page_no': 6, 'bbox': {'l': 515.925, 't': 723.223, 'r': 542.625, 'b': 716.353, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 7.0]}]",
text,Target,"[{'page_no': 6, 'bbox': {'l': 519.129, 't': 714.859, 'r': 539.277, 'b': 707.989, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,Start,"[{'page_no': 6, 'bbox': {'l': 55.061, 't': 692.575, 'r': 63.649, 'b': 673.198, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 5.0]}]",
text,End,"[{'page_no': 6, 'bbox': {'l': 55.151, 't': 635.463, 'r': 63.739, 'b': 619.413, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,(a) Real world tasks visualization,"[{'page_no': 6, 'bbox': {'l': 244.236, 't': 588.767, 'r': 367.766, 'b': 581.07, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 34.0]}]",
text,Out-of-distribution tasks,"[{'page_no': 6, 'bbox': {'l': 355.873, 't': 578.11, 'r': 424.766, 'b': 572.136, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 25.0]}]",
text,In-distribution tasks,"[{'page_no': 6, 'bbox': {'l': 176.274, 't': 578.11, 'r': 232.847, 'b': 572.136, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 21.0]}]",
text,(b) Quantitative evaluation.,"[{'page_no': 6, 'bbox': {'l': 256.076, 't': 517.738, 'r': 355.926, 'b': 510.041, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 28.0]}]",
text,"base LLaMA-8B ( +58.4% ). Even in long-term tasks such as Stack Blocks and Empty Container , LLaMA-8B trained with BLAZER outperforms LLaMA-70B in Stack Blocks by 14% and is almost on par with it on the Empty Container task. We notice that LLaMA-70B shows remarkable results with our detailed prompt, outperforming Code-as-Policy and VoxPoser, and proving the strength of our baseline. The MALMM multi-agent framework still surpasses LLaMA70B due to its multistep failure detection and correction approach, although it still falls short of our agent.","[{'page_no': 6, 'bbox': {'l': 54.0, 't': 441.664, 'r': 298.801, 'b': 325.127, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 550.0]}]",
text,"Interestingly, we also note that zero-shot usage of LLaMA8B results in unsatisfactory performance (avg. 25.3% success). In particular, for Stack Blocks and Empty Container , LLaMA-8B often fails to generate valid control commands, even for a single episode in Stack Blocks . In contrast, LLaMA-70B successfully handles most tasks. This result is consistent with recent studies on long-context generation in large language models [47], [48]. Ultimately, the ability of LLaMA-70B to provide more successful examples for D BLAZER justifies its usage as LLMboot.","[{'page_no': 6, 'bbox': {'l': 54.0, 't': 321.494, 'r': 298.801, 'b': 204.471, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 558.0]}]",
text,"Perception impact. We now evaluate the robustness of BLAZER to a more realistic setup with visual observations replacing ground truth object states. We employ our visionbased pipeline (Section III-C) to process input views for simulated tasks in Table I, and obtain ˜ Σ E , which we use as input for LLMs. For fair comparison, we equip LLaMA-70B and the base LLaMA-8B as well as BLAZER with the same vision pipeline and report results in Figure 4. We observe that even with the noisy state estimation of our vision pipeline, LLaMA-8B trained with BLAZER still outperforms alternatives. In particular, the gain with respect to LLaMA-70B ( +15% ) is even higher than when providing the ground truth","[{'page_no': 6, 'bbox': {'l': 54.0, 't': 198.85400000000004, 'r': 298.801, 'b': 58.39099999999996, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 696.0]}]",
text,"Σ E ( +6.2% , see Table I). This shows the robustness to noise in the policies generated with LLMs trained with BLAZER.","[{'page_no': 6, 'bbox': {'l': 313.2, 't': 441.664, 'r': 558.001, 'b': 420.769, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 119.0]}]",
section_header,C. Robot experiments,"[{'page_no': 6, 'bbox': {'l': 313.2, 't': 409.127, 'r': 402.545, 'b': 400.539, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 20.0]}]",1.0
text,We next deploy BLAZER in the real robot setup and compare its performance to LLaMA-70B baseline while using the same vision pipeline for both methods.,"[{'page_no': 6, 'bbox': {'l': 313.2, 't': 393.158, 'r': 558.001, 'b': 360.695, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 150.0]}]",
text,"Generalization capabilities. We use 9 additional tasks, shown in Figure 5a, to assess the transferability of BLAZER to real manipulation scenarios. We first consider Indistribution tasks that resemble the Stack Blocks , Put Block , and Rubbish in Bin tasks in Figure 3. We use these tasks to quantify the transfer of tasks in T in real-world deployment. We also propose 6 new Out-of-distribution tasks, different from those in T : Block in Right/Left Basket , Sweet/Sour Fruit in Bowl , Fruit in Closest Basket , Cup on Colored Object , Jar in Bin , and Case on Target . By testing these tasks, we aim to demonstrate the ability of BLAZER to solve tasks beyond those in T . We average results over 10 episodes for each task, replacing generic attributes in the task description (e.g. 'colored', 'sweet/sour') with precise values (e.g. 'red', 'sour'), randomized for each episode. From the results in Table 5b, we show that our LLM agent trained with BLAZER still outperforms the baseline , proving that the capabilities of tasks in T transfer successfully to the real world. In particular, for in-distribution tasks, we notice a higher Stack Blocks performance, that was suboptimal in Figure 4. We speculate that the performance in simulation could have been influenced mainly by the presence of distractor elements (see Figure 3). For out-of-distribution tasks, we beat LLaMA-70B on 5 tasks out of 6, ultimately proving that the benefits of BLAZER training transfer to new task in real world settings.","[{'page_no': 6, 'bbox': {'l': 313.2, 't': 354.271, 'r': 558.001, 'b': 58.40700000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1502.0]}]",
text,Start,"[{'page_no': 7, 'bbox': {'l': 49.299, 't': 626.069, 'r': 61.118, 'b': 595.278, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 5.0]}]",
text,End,"[{'page_no': 7, 'bbox': {'l': 49.299, 't': 514.642, 'r': 61.118, 'b': 496.163, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,Count the number of animals and,"[{'page_no': 7, 'bbox': {'l': 78.611, 't': 705.033, 'r': 177.112, 'b': 699.123, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 31.0]}]",
text,place the correct numbered block,"[{'page_no': 7, 'bbox': {'l': 77.071, 't': 698.425, 'r': 178.649, 'b': 692.516, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 32.0]}]",
text,in the basket.,"[{'page_no': 7, 'bbox': {'l': 104.798, 't': 691.817, 'r': 147.895, 'b': 685.908, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 14.0]}]",
text,We are X. Pick a cross block,"[{'page_no': 7, 'bbox': {'l': 329.592, 't': 705.033, 'r': 415.781, 'b': 699.123, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 28.0]}]",
text,from the blue notebook and place,"[{'page_no': 7, 'bbox': {'l': 323.43, 't': 698.425, 'r': 425.009, 'b': 692.516, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 32.0]}]",
text,it in the empty tic-tac-toe cell,"[{'page_no': 7, 'bbox': {'l': 323.43, 't': 691.817, 'r': 425.009, 'b': 685.908, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 32.0]}]",
text,that gives an immediate win for,"[{'page_no': 7, 'bbox': {'l': 324.971, 't': 685.21, 'r': 423.472, 'b': 679.3, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 31.0]}]",
text,X.,"[{'page_no': 7, 'bbox': {'l': 369.642, 't': 678.602, 'r': 375.803, 'b': 672.693, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,Make the stuffed animal win the,"[{'page_no': 7, 'bbox': {'l': 448.067, 't': 705.032, 'r': 543.49, 'b': 699.123, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 31.0]}]",
text,race.,"[{'page_no': 7, 'bbox': {'l': 488.117, 't': 698.424, 'r': 503.513, 'b': 692.515, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 5.0]}]",
text,Complete the equation by placing,"[{'page_no': 7, 'bbox': {'l': 199.743, 't': 701.729, 'r': 298.244, 'b': 695.82, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 32.0]}]",
text,the correct block on the target.,"[{'page_no': 7, 'bbox': {'l': 199.743, 't': 695.121, 'r': 298.244, 'b': 689.212, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 32.0]}]",
text,Count Animals,"[{'page_no': 7, 'bbox': {'l': 92.015, 't': 735.454, 'r': 160.62, 'b': 725.314, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 13.0]}]",
text,Win Tic-tac-toe,"[{'page_no': 7, 'bbox': {'l': 332.801, 't': 735.454, 'r': 411.959, 'b': 725.314, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 15.0]}]",
text,Animal Race,"[{'page_no': 7, 'bbox': {'l': 467.01, 't': 735.454, 'r': 525.061, 'b': 725.314, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 11.0]}]",
text,Solve Equation,"[{'page_no': 7, 'bbox': {'l': 211.627, 't': 735.454, 'r': 285.508, 'b': 725.314, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 14.0]}]",
text,"Qualitative examples. To further evaluate the generalization abilities of BLAZER, we present it with four tasks that require high-level reasoning. As in previous experiments in Fig. 5b, we use BLAZER trained in simulation on T tasks together with the vision pipeline described in Section III-C. Specifically, we employ numbered blocks to test math capabilities in the Count Animals and Solve Equation tasks, game strategic planning in Win Tic-tac-toe , and contextual awareness in Animal Race . We provide qualitative results of BLAZER for these tasks in Figure 6. As can be seen, BLAZER can successfully solve tasks that require highlevel reasoning while being substantially different from the training tasks T . Videos illustrating execution of these tasks can be found on the project website [1].","[{'page_no': 7, 'bbox': {'l': 54.0, 't': 384.348, 'r': 298.801, 'b': 219.99, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 799.0]}]",
section_header,D. Ablation studies,"[{'page_no': 7, 'bbox': {'l': 54.0, 't': 208.76800000000003, 'r': 134.149, 'b': 200.18000000000006, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 19.0]}]",1.0
text,"In this section, we propose ablation studies. First, we study the impact of changing the number of per-task training samples N that we use for training LLMBLAZER. Second, we analyze if smaller models are compatible with BLAZER.","[{'page_no': 7, 'bbox': {'l': 54.0, 't': 192.971, 'r': 298.801, 'b': 148.55399999999997, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 227.0]}]",
text,"Training dataset size. In BLAZER, we can generate arbitrarily large datasets of training samples. To understand the impact of the data scale, we train LLaMA-8B with BLAZER on 4 different datasets generated automatically, containing 500, 1000, 2000, and 4000 samples per task ( N ). We report in Figure 7 (left) the average results across the 9 tasks in simulation that we use in Table I. From our results,","[{'page_no': 7, 'bbox': {'l': 54.0, 't': 139.07799999999997, 'r': 298.801, 'b': 58.40700000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 405.0]}]",
text,0.5K1K,"[{'page_no': 7, 'bbox': {'l': 345.952, 't': 298.195, 'r': 367.917, 'b': 291.263, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 6.0]}]",
text,2K,"[{'page_no': 7, 'bbox': {'l': 383.108, 't': 298.195, 'r': 390.795, 'b': 291.263, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,4K,"[{'page_no': 7, 'bbox': {'l': 428.864, 't': 298.195, 'r': 436.551, 'b': 291.263, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,Number of samples,"[{'page_no': 7, 'bbox': {'l': 360.12, 't': 290.738, 'r': 425.212, 'b': 283.806, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 17.0]}]",
text,60,"[{'page_no': 7, 'bbox': {'l': 329.017, 't': 304.493, 'r': 336.585, 'b': 297.562, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,70,"[{'page_no': 7, 'bbox': {'l': 329.017, 't': 323.153, 'r': 336.585, 'b': 316.221, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,80,"[{'page_no': 7, 'bbox': {'l': 329.017, 't': 341.812, 'r': 336.585, 'b': 334.88, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,90,"[{'page_no': 7, 'bbox': {'l': 329.017, 't': 360.471, 'r': 336.585, 'b': 353.539, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,100,"[{'page_no': 7, 'bbox': {'l': 325.232, 't': 379.13, 'r': 336.584, 'b': 372.198, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 3.0]}]",
text,Success rate (%),"[{'page_no': 7, 'bbox': {'l': 316.769, 't': 366.569, 'r': 323.701, 'b': 310.509, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 16.0]}]",
text,72.2,"[{'page_no': 7, 'bbox': {'l': 341.234, 't': 338.383, 'r': 356.37, 'b': 330.461, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 4.0]}]",
text,82.7,"[{'page_no': 7, 'bbox': {'l': 356.498, 't': 357.364, 'r': 371.634, 'b': 349.442, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 4.0]}]",
text,83.2,"[{'page_no': 7, 'bbox': {'l': 379.376, 't': 358.297, 'r': 394.512, 'b': 350.375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 4.0]}]",
text,83.7,"[{'page_no': 7, 'bbox': {'l': 424.315, 't': 359.23, 'r': 441.097, 'b': 351.308, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 4.0]}]",
text,83.7,"[{'page_no': 7, 'bbox': {'l': 425.132, 't': 359.23, 'r': 440.268, 'b': 351.308, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 4.0]}]",
text,Impact of,"[{'page_no': 7, 'bbox': {'l': 370.573, 't': 385.66, 'r': 409.002, 'b': 376.748, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 9.0]}]",
text,N,"[{'page_no': 7, 'bbox': {'l': 409.001, 't': 385.66, 'r': 414.723, 'b': 376.748, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1.0]}]",
text,1B,"[{'page_no': 7, 'bbox': {'l': 457.386, 't': 298.195, 'r': 465.252, 'b': 291.263, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,3B,"[{'page_no': 7, 'bbox': {'l': 480.264, 't': 298.195, 'r': 488.13, 'b': 291.263, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,8B,"[{'page_no': 7, 'bbox': {'l': 537.459, 't': 298.195, 'r': 545.325, 'b': 291.263, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 2.0]}]",
text,Parameter size,"[{'page_no': 7, 'bbox': {'l': 476.166, 't': 290.738, 'r': 526.527, 'b': 283.806, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 14.0]}]",
text,63.5,"[{'page_no': 7, 'bbox': {'l': 466.42, 't': 312.199, 'r': 481.557, 'b': 304.277, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 4.0]}]",
text,84.9,"[{'page_no': 7, 'bbox': {'l': 475.805, 't': 361.469, 'r': 492.587, 'b': 353.547, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 4.0]}]",
text,83.2,"[{'page_no': 7, 'bbox': {'l': 533.817, 't': 358.297, 'r': 548.953, 'b': 350.375, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 4.0]}]",
text,Impact of model size,"[{'page_no': 7, 'bbox': {'l': 461.468, 't': 385.52, 'r': 541.255, 'b': 376.608, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 20.0]}]",
text,"it is evident that the size of N significantly impacts the efficacy of the trained LLMBLAZER, with a substantial loss of performance when N = 500 (success rate 72.2% ). However, we also noticed diminishing returns at the increase of N . In particular, we noticed that the performance doubling N from 2000 to 4000 is only +0.5% , so we have chosen N = 2000 in the paper to shorten training times. In conclusion, we advocate that although BLAZER is effective in exploiting the synthetically generated data, more strategies may be needed to increase accuracy beyond saturation.","[{'page_no': 7, 'bbox': {'l': 313.2, 't': 174.87400000000002, 'r': 558.004, 'b': 58.40700000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 574.0]}]",
text,"Model size. Within BLAZER, we finetune a small model (LLaMA-8B) and use a larger one for data generation (LLaMA-70B). We aim to understand whether even smaller models can be trained with BLAZER, to enable applications on edge devices with low computational resources. To do so, we trained LLaMA-3.2 1B and LLaMA-3.2 3B [49] with BLAZER and compared it with our finetuned LLaMA-8B used for all other experiments. All models use data generated by LLaMA-70B as LLMboot. We present results in Figure 7 (right). The usage of the 3B and 1B models still results in remarkable performance. Interestingly, LLaMA-3.2 3B achieves 84.9% as average success rate, even marginally higher than LLaMA-8B ( 83.2% ). Please note that LLaMA3.2 is a different release from LLaMA-8B (3.1), therefore, we attribute the higher performance to the superior data quality used for the 3.2 LLaMA release [49]. This shows that even a compact model used as LLMBLAZER can result in competitive performance with state-of-the-art zero-shot manipulation methods based on LLMs. Conversely, the 1B model yields lower successes (63.5%), but still beats some baselines in Table I with a very limited number of parameters. This shows further flexibility of BLAZER in model size for applications with significant computational constraints.","[{'page_no': 8, 'bbox': {'l': 54.0, 't': 736.837, 'r': 298.805, 'b': 464.883, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 1298.0]}]",
section_header,V. CONCLUSIONS,"[{'page_no': 8, 'bbox': {'l': 138.194, 't': 454.375, 'r': 214.615, 'b': 445.823, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 14.0]}]",1.0
text,"In this paper, we introduced BLAZER, a method for finetuning standard LLMs to obtain specialized agents for robotic manipulation. We demonstrated the efficacy of BLAZER in both simulated and real environments and evaluated its generalization performance on different tasks beyond those used for training. We believe that our work will encourage further research on the usage of pretrained LLMs for robotics-oriented tasks. While BLAZER currently exploits only positive demonstrations for supervised finetuning, future work can benefit from both positive and negative samples to learn more accurate manipulation models. Indeed, unsuccessful episodes generated by our method in simulation could be used for more advanced post-training techniques such as Direct Preference Optimization (DPO) [50], that are able to exploit preference pairs including negative examples.","[{'page_no': 8, 'bbox': {'l': 54.0, 't': 439.014, 'r': 298.801, 'b': 263.08899999999994, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 865.0]}]",
section_header,REFERENCES,"[{'page_no': 8, 'bbox': {'l': 148.505, 't': 252.58100000000002, 'r': 204.302, 'b': 244.029, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 10.0]}]",1.0
list_item,"'BLAZER project webpage,' https://blazer-llm-agent.github.io/. 1, 7","[{'page_no': 8, 'bbox': {'l': 57.985, 't': 236.22199999999998, 'r': 297.12, 'b': 229.38, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 71.0]}]",
list_item,"M. J. Kim et al. , 'OpenVLA: An open-source vision-language-action model,' arXiv , 2024. 1, 2","[{'page_no': 8, 'bbox': {'l': 57.985, 't': 227.37400000000002, 'r': 298.802, 'b': 211.42200000000003, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 97.0]}]",
list_item,"P. Intelligence et al. , ' π 0.5: a vision-language-action model with openworld generalization,' arXiv , 2025. 1, 2","[{'page_no': 8, 'bbox': {'l': 57.985, 't': 209.52800000000002, 'r': 298.802, 'b': 193.46500000000003, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 119.0]}]",
list_item,"T. L. Team et al. , 'A careful examination of large behavior models for multitask dexterous manipulation,' arXiv , 2025. 1, 2","[{'page_no': 8, 'bbox': {'l': 57.985, 't': 191.45799999999997, 'r': 298.802, 'b': 175.50700000000006, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 129.0]}]",
list_item,"J. Lee, J. Duan, H. Fang et al. , 'MolmoAct: Action reasoning models that can reason in space,' arXiv , 2025. 1","[{'page_no': 8, 'bbox': {'l': 57.985, 't': 173.50099999999998, 'r': 298.802, 'b': 157.54899999999998, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 115.0]}]",
list_item,"A. Padalkar et al. , 'Open X-Embodiment: Robotic learning datasets and rt-x models,' arXiv , 2023. 1, 2","[{'page_no': 8, 'bbox': {'l': 57.985, 't': 155.543, 'r': 298.802, 'b': 139.59199999999998, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 107.0]}]",
list_item,"R. McCarthy, D. C. Tan, D. Schmidt, F. Acero, N. Herr, Y. Du, T. G. Thuruthel, and Z. Li, 'Towards generalist robot learning from internet video: A survey,' JAIR , 2024. 1","[{'page_no': 8, 'bbox': {'l': 57.985, 't': 137.44299999999998, 'r': 298.802, 'b': 112.668, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 175.0]}]",
list_item,"L. Wang, Y. Ling, Z. Yuan, M. Shridhar, C. Bao, Y. Qin, B. Wang, H. Xu, and X. Wang, 'GenSim: Generating robotic simulation tasks via large language models,' arXiv , 2023. 1, 2, 3","[{'page_no': 8, 'bbox': {'l': 57.985, 't': 110.519, 'r': 298.802, 'b': 85.74400000000003, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 183.0]}]",
list_item,"P. Hua, M. Liu, A. Macaluso, Y. Lin, W. Zhang, H. Xu, and L. Wang, 'GenSim2: Scaling robot data generation with multi-modal and reasoning llms,' in CoRL , 2024. 1, 2, 3","[{'page_no': 8, 'bbox': {'l': 57.985, 't': 83.59500000000003, 'r': 298.802, 'b': 58.82000000000005, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 172.0]}]",
list_item,"Y. Wang, Z. Xian, F. Chen, T.-H. Wang, Y. Wang, Z. Erickson, D. Held, and C. Gan, 'RoboGen: Towards unleashing infinite data for automated robot learning via generative simulation,' ICML , 2024. 1","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 735.151, 'r': 558.002, 'b': 701.41, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 201.0]}]",
list_item,"J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. R. Florence, and A. Zeng, 'Code as policies: Language model programs for embodied control,' ICRA , 2022. 1, 2, 5","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 698.688, 'r': 558.002, 'b': 673.913, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 178.0]}]",
list_item,"J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, F. Xia, Q. Le, and D. Zhou, 'Chain of thought prompting elicits reasoning in large language models,' NeurIPS , 2022. 1, 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 671.191, 'r': 558.002, 'b': 646.417, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 179.0]}]",
list_item,"E. Zelikman, Y. Wu, and N. D. Goodman, 'Star: Bootstrapping reasoning with reasoning,' in NeurIPS , 2022. 1, 2, 4","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 643.694, 'r': 558.002, 'b': 627.886, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 118.0]}]",
list_item,"H. Lin, Z. Sun, Y. Yang, and S. Welleck, 'Lean-STaR: Learning to interleave thinking and proving,' ICLR , 2025. 1","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 625.164, 'r': 558.002, 'b': 609.356, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 118.0]}]",
list_item,"S. R. Motwani, C. Smith, R. J. Das, M. Rybchuk, P. Torr, I. Laptev, F. Pizzati, R. Clark, and C. S. de Witt, 'MALT: Improving reasoning with multi-agent llm training,' COLM , 2025. 1, 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 606.633, 'r': 558.002, 'b': 581.859, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 190.0]}]",
list_item,"H. Singh, R. J. Das, M. Han, P. Nakov, and I. Laptev, 'MALMM: Multi-agent large language models for zero-shot robotics manipulation,' IROS , 2025. 2, 3, 4, 5","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 579.136, 'r': 558.002, 'b': 554.362, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 162.0]}]",
list_item,"J. Lee, J. Duan, H. Fang, Y. Deng, S. Liu, B. Li, B. Fang, J. Zhang, Y. R. Wang, S. Lee, W. Han, W. Pumacay, A. Wu, R. Hendrix, K. Farley, E. VanderBilt, A. Farhadi, D. Fox, and R. Krishna, 'Molmo and pixmo: Open weights and open data for state-of-the-art visionlanguage models,' CVPR , 2024. 2, 4","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 551.64, 'r': 558.002, 'b': 508.932, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 302.0]}]",
list_item,"W. Yuan, A. Murali, A. Mousavian, and D. Fox, 'M2T2: Multi-task masked transformer for object-centric pick and place,' in CoRL , 2023. 2, 4","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 506.21, 'r': 558.002, 'b': 481.435, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 144.0]}]",
list_item,"S. Levine, C. Finn, T. Darrell, and P. Abbeel, 'End-to-end training of deep visuomotor policies,' JMLR , 2015. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 478.713, 'r': 558.002, 'b': 462.905, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 117.0]}]",
list_item,"P. R. Florence, C. Lynch, A. Zeng, O. Ramirez, A. Wahid, L. Downs, A. S. Wong, J. Lee, I. Mordatch, and J. Tompson, 'Implicit behavioral cloning,' in CoRL , 2022. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 460.183, 'r': 558.002, 'b': 435.408, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 169.0]}]",
list_item,"P.-L. Guhur, S. Chen, R. G. Pinel, M. Tapaswi, I. Laptev, and C. Schmid, 'Instruction-driven history-aware policies for robotic manipulations,' in CoRL , 2023. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 432.686, 'r': 558.002, 'b': 407.911, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 166.0]}]",
list_item,"A. Brohan et al. , 'RT-2: Vision-language-action models transfer web knowledge to robotic control,' arXiv , 2023. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 405.332, 'r': 558.002, 'b': 389.381, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 120.0]}]",
list_item,"X. Li et al. , 'ManipLLM: Embodied multimodal large language model for object-centric robotic manipulation,' CVPR , 2023. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 386.801, 'r': 558.002, 'b': 370.85, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 128.0]}]",
list_item,"W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. Fei-Fei, 'VoxPoser: Composable 3d value maps for robotic manipulation with language models,' CoRL , 2023. 2, 5","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 368.128, 'r': 558.002, 'b': 343.353, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 165.0]}]",
list_item,"J. Duan, W. Pumacay, N. Kumar, Y. R. Wang, S. Tian, W. Yuan, R. Krishna, D. Fox, A. Mandlekar, and Y. Guo, 'AHA: A visionlanguage-model for detecting and reasoning over failures in robotic manipulation,' ICLR , 2025. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 340.631, 'r': 558.002, 'b': 306.89, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 223.0]}]",
list_item,"M. Nye, A. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, C. Sutton, and A. Odena, 'Show your work: Scratchpads for intermediate computation with language models,' arXiv , 2021. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 304.168, 'r': 558.002, 'b': 270.427, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 242.0]}]",
list_item,"W. Chen, X. Ma, X. Wang, and W. W. Cohen, 'Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks,' TMLR , 2022. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 267.7049999999999, 'r': 558.002, 'b': 242.93000000000006, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 166.0]}]",
list_item,"DeepSeek-AI, D. Guo et al. , 'Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,' arXiv , 2025. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 240.351, 'r': 558.002, 'b': 224.399, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 135.0]}]",
list_item,"A. R. Setlur, S. Garg, X. Geng, N. Garg, V. Smith, and A. Kumar, 'Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold,' NeurIPS , 2024. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 221.67700000000002, 'r': 558.002, 'b': 196.90200000000004, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 179.0]}]",
list_item,"N. Ho, L. Schmid, and S.-Y. Yun, 'Large language models are reasoning teachers,' in ACL , 2022. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 194.18000000000006, 'r': 558.002, 'b': 178.37200000000007, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 102.0]}]",
list_item,"L. Pinto and A. K. Gupta, 'Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours,' ICRA , 2015. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 175.64999999999998, 'r': 558.002, 'b': 159.841, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 131.0]}]",
list_item,"P. Agrawal, A. Nair, P. Abbeel, J. Malik, and S. Levine, 'Learning to poke by poking: Experiential learning of intuitive physics,' NeurIPS , 2016. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 157.11900000000003, 'r': 558.002, 'b': 132.34400000000005, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 153.0]}]",
list_item,"Z. Liu, A. Bahety, and S. Song, 'Reflect: Summarizing robot experiences for failure explanation and correction,' CoRL , 2023. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 129.62200000000007, 'r': 558.002, 'b': 113.81399999999996, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 132.0]}]",
list_item,"Y. Guo, Y.-J. Wang, L. Zha, Z. Jiang, and J. Chen, 'DoReMi: Grounding language model by detecting and recovering from planexecution misalignment,' in IROS , 2024. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 111.09199999999998, 'r': 558.002, 'b': 86.31700000000001, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 169.0]}]",
list_item,"T. V. Vo, T. Q. Nguyen, K. Nguyen, D. H. M. Nguyen, and M. N. Vu, 'ReFineVLA: Reasoning-aware teacher-guided transfer fine-tuning,' arxiv , 2025. 2","[{'page_no': 8, 'bbox': {'l': 313.2, 't': 83.59500000000003, 'r': 558.002, 'b': 58.82000000000005, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 152.0]}]",
list_item,"G. Team, 'Gemini: A family of highly capable multimodal models,' 2025. 2","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 735.152, 'r': 298.802, 'b': 719.343, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 77.0]}]",
list_item,"C. Li, J. Liu, G. Wang, X. Li, S. Chen, L. Heng, C. Xiong, J. Ge, R. Zhang, K. Zhou, and S. Zhang, 'A self-correcting vision-languageaction model for fast and slow system manipulation,' arxiv , 2024. 2","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 717.219, 'r': 298.802, 'b': 692.444, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 206.0]}]",
list_item,"G. Tang, S. Rajkumar, Y. Zhou, H. R. Walke, S. Levine, and K. Fang, 'Kalie: Fine-tuning vision-language models for open-world manipulation without robot data,' ICRA , 2024. 2","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 690.32, 'r': 298.802, 'b': 665.545, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 179.0]}]",
list_item,"J. Gao, B. Sarkar, F. Xia, T. Xiao, J. Wu, B. Ichter, A. Majumdar, and D. Sadigh, 'Physically grounded vision-language models for robotic manipulation,' ICRA , 2023. 2","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 663.421, 'r': 298.802, 'b': 638.646, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 172.0]}]",
list_item,"X. Li, C. Mata, J. S. Park, K. Kahatapitiya, Y. S. Jang, J. Shang, K. Ranasinghe, R. Burgert, M. Cai, Y. J. Lee, and M. S. Ryoo, 'LLaRA: Supercharging robot learning data for vision-language policy,' ICLR , 2025. 2","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 636.522, 'r': 298.802, 'b': 602.78, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 219.0]}]",
list_item,"Z. Xue, S. Deng, Z. Chen, Y. Wang, Z. Yuan, and H. Xu, 'Demogen: Synthetic demonstration generation for data-efficient visuomotor policy learning,' RSS , 2025. 3","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 600.656, 'r': 298.802, 'b': 575.881, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 166.0]}]",
list_item,"OpenAI, 'GPT-4 technical report,' 2023. 3, 5","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 573.757, 'r': 222.711, 'b': 566.915, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 49.0]}]",
list_item,"A. Torralba and A. A. Efros, 'Unbiased look at dataset bias,' in CVPR , 2011. 4","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 564.934, 'r': 298.802, 'b': 548.982, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 84.0]}]",
list_item,"A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Doll´ ar, and R. B. Girshick, 'Segment anything,' in ICCV , 2023. 4","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 546.858, 'r': 298.802, 'b': 522.083, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 187.0]}]",
list_item,"S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, 'Rlbench: The robot learning benchmark & learning environment,' RA-L , 2019. 4, 5","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 519.959, 'r': 298.802, 'b': 504.15, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 136.0]}]",
list_item,"J. Elsner, 'Taming the panda with python: A powerful duo for seamless robotics programming and integration,' SoftwareX , 2023. 4","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 502.026, 'r': 298.802, 'b': 486.218, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 133.0]}]",
list_item,"C. An, J. Zhang, M. Zhong, L. Li, S. Gong, Y. Luo, J. Xu, and L. Kong, 'Why does the effective context length of llms fall short?' ICLR , 2025. 6","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 484.093, 'r': 298.802, 'b': 459.319, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 150.0]}]",
list_item,"C.-P. Hsieh, S. Sun, S. Kriman, S. Acharya, D. Rekesh, F. Jia, and B. Ginsburg, 'Ruler: What's the real context size of your long-context language models?' COLM , 2024. 6","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 457.194, 'r': 298.802, 'b': 432.419, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 175.0]}]",
list_item,"Meta AI, 'Llama 3.2: Revolutionizing edge ai and vision with open, customizable models,' 2024. 8","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 430.295, 'r': 298.802, 'b': 414.487, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 101.0]}]",
list_item,"R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn, 'Direct preference optimization: Your language model is secretly a reward model,' NeurIPS , 2023. 8","[{'page_no': 9, 'bbox': {'l': 54.0, 't': 412.362, 'r': 298.802, 'b': 387.588, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 179.0]}]",
section_header,APPENDIX,"[{'page_no': 10, 'bbox': {'l': 282.972, 't': 736.837, 'r': 329.035, 'b': 727.881, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 8.0]}]",1.0
text,"This appendix provides additional details about our BLAZER framework, including LLM Agent Prompt in Appendix-A, and RLBench tasks in Appendix-B.","[{'page_no': 10, 'bbox': {'l': 54.0, 't': 718.001, 'r': 557.998, 'b': 697.494, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 144.0]}]",
section_header,A. Prompts,"[{'page_no': 10, 'bbox': {'l': 54.0, 't': 682.794, 'r': 100.874, 'b': 674.206, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 10.0]}]",1.0
text,"The prompt used by the LLM Agent is illustrated in Figure 8. It contains four placeholders: [INSERT TASK] representing the task instruction, [INSERT EE POSITION] denoting the initial position of the end effector, [INSERT EE ORIENTATION] specifying its initial orientation, and [INSERT CURRENT STATE ENVIRONMENT] indicating the current state or observation of the environment.","[{'page_no': 10, 'bbox': {'l': 54.0, 't': 665.744, 'r': 557.998, 'b': 621.147, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 375.0]}]",
section_header,LLM Agent Prompt,"[{'page_no': 10, 'bbox': {'l': 249.432, 't': 597.881, 'r': 362.662, 'b': 584.982, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 16.0]}]",1.0
text,"You are a sentient AI specialized in generating a sequence of steps and Python code for the robot arm end-effector to complete a given task. The end effector is in a enviroment and informations about the objects in the environment including the end-effector are provided in terms of their positions and orientations. You must remember that this conversation is a monologue, and that you are in control. I am not able to assist you with any questions, and you must output the plan and code yourself by making use of the common sense, general knowledge, and available information.","[{'page_no': 10, 'bbox': {'l': 81.833, 't': 577.641, 'r': 532.925, 'b': 553.488, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 578.0]}]",
section_header,PLANNER:,"[{'page_no': 10, 'bbox': {'l': 81.833, 't': 546.106, 'r': 105.153, 'b': 540.869, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 8.0]}]",1.0
section_header,ENVIRONMENT SET-UP:,"[{'page_no': 10, 'bbox': {'l': 93.471, 't': 539.803, 'r': 149.1, 'b': 534.566, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 19.0]}]",1.0
text,The 3D coordinate system of the environment is as follows:,"[{'page_no': 10, 'bbox': {'l': 93.471, 't': 533.499, 'r': 263.153, 'b': 528.262, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 58.0]}]",
list_item,"The x-axis is in the horizontal direction, increasing to the right.","[{'page_no': 10, 'bbox': {'l': 105.206, 't': 527.195, 'r': 309.986, 'b': 521.958, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 70.0]}]",
text,"2. The y-axis is in the depth direction, increasing away from you.","[{'page_no': 10, 'bbox': {'l': 105.206, 't': 520.891, 'r': 298.36, 'b': 515.654, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 66.0]}]",
list_item,"The z-axis is in the vertical direction, increasing upwards.","[{'page_no': 10, 'bbox': {'l': 105.206, 't': 514.588, 'r': 289.546, 'b': 509.351, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 63.0]}]",
list_item,Workspace is in the table positioned at a z-level of 0.75199986. The workspace ranges from x: -0.075 to 0.575 and y: -0.455 to 0.455.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 508.284, 'r': 503.58, 'b': 503.047, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 136.0]}]",
section_header,CONSTRAINTS:,"[{'page_no': 10, 'bbox': {'l': 93.471, 't': 495.676, 'r': 128.511, 'b': 490.44, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 12.0]}]",1.0
list_item,**Orientation of the end effector** will always be **z orientation of object to grasp or destination to place**.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 489.373, 'r': 444.596, 'b': 484.136, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 115.0]}]",
list_item,"Negative rotation values represent clockwise rotation, and positive rotation values represent anticlockwise rotation. The rotation values should be in radians.","[{'page_no': 10, 'bbox': {'l': 81.833, 't': 483.069, 'r': 518.301, 'b': 471.528, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 162.0]}]",
list_item,The <safe distance> in the z direction is 0.1 units.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 470.462, 'r': 266.074, 'b': 465.225, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 55.0]}]",
list_item,"The <release distance> in the z direction is 0.02 units above the top surface of the destination area or object, please estimate this distance.","[{'page_no': 10, 'bbox': {'l': 81.833, 't': 464.158, 'r': 506.578, 'b': 452.617, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 146.0]}]",
list_item,"**Remember you can only grasp the object from its *CENTER*. Not from any other position. So to grasp the object, end effector has to be lowered down to center of object**","[{'page_no': 10, 'bbox': {'l': 81.833, 't': 451.55, 'r': 512.391, 'b': 440.01, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 173.0]}]",
section_header,COLLISION AVOIDANCE:,"[{'page_no': 10, 'bbox': {'l': 93.471, 't': 432.639, 'r': 151.952, 'b': 427.402, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 20.0]}]",1.0
text,If there are multiple objects in the environment:,"[{'page_no': 10, 'bbox': {'l': 93.471, 't': 426.35, 'r': 236.832, 'b': 421.094, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 49.0]}]",
list_item,"Make sure to consider the widths, lengths, and heights of other objects so that robot arm end effector does not collide with other objects or table.","[{'page_no': 10, 'bbox': {'l': 81.833, 't': 420.046, 'r': 529.942, 'b': 408.467, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 151.0]}]",
list_item,This information may help to generate additional trajectories and add specific waypoints (calculated from the given objects' information) to avoid collision with other objects and the table.,"[{'page_no': 10, 'bbox': {'l': 81.833, 't': 407.4, 'r': 527.017, 'b': 395.859, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 193.0]}]",
section_header,"COLLISION FREE OBJECT INTERACTION RULES:,","[{'page_no': 10, 'bbox': {'l': 93.471, 't': 388.489, 'r': 216.366, 'b': 383.252, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 41.0]}]",1.0
list_item,Position the gripper <safe distance> above the target object.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 382.185, 'r': 292.421, 'b': 376.948, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 64.0]}]",
list_item,"Move to the **CENTER** of the target object. If *CENTER* position is (x,y,z) then grasping posiotion will also be (x,y,z) Do not add any height margin.","[{'page_no': 10, 'bbox': {'l': 81.833, 't': 375.881, 'r': 515.297, 'b': 364.341, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 154.0]}]",
text,3. Grasp the target object.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 363.274, 'r': 184.221, 'b': 358.037, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 27.0]}]",
list_item,Raise the gripper <safe distance> above the target object.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 356.97, 'r': 283.702, 'b': 351.733, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 61.0]}]",
list_item,Move to <safe distance>  above the destination area.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 350.667, 'r': 266.074, 'b': 345.43, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 55.0]}]",
list_item,Lower the gripper to the destination area.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 344.363, 'r': 236.82, 'b': 339.126, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 45.0]}]",
list_item,Release the object (drop) at <release distance>  (**(0.02 units)**) above the top surface of destination area.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 338.059, 'r': 435.877, 'b': 332.822, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 113.0]}]",
list_item,Raise the gripper <safe distance> above the destination area.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 331.755, 'r': 292.421, 'b': 326.518, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 64.0]}]",
section_header,PLANNING:,"[{'page_no': 10, 'bbox': {'l': 93.471, 't': 319.148, 'r': 119.792, 'b': 313.911, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 9.0]}]",1.0
list_item,"Describe the relative positions of all objects in the environment, including their spatial relationships, alignments, and groupings.","[{'page_no': 10, 'bbox': {'l': 105.206, 't': 312.844, 'r': 500.673, 'b': 307.607, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 135.0]}]",
list_item,"Provide a detailed, step-by-step plan for the given task.","[{'page_no': 10, 'bbox': {'l': 105.206, 't': 306.54, 'r': 280.918, 'b': 301.304, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 60.0]}]",
list_item,Generate the all code corresponding to each high level plan.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 300.237, 'r': 289.515, 'b': 295.0, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 63.0]}]",
section_header,CODE GENERATOR:,"[{'page_no': 10, 'bbox': {'l': 81.833, 't': 287.644, 'r': 125.608, 'b': 282.387, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 15.0]}]",1.0
text,AVAILABLE FUNCTIONS:,"[{'page_no': 10, 'bbox': {'l': 93.471, 't': 281.309, 'r': 151.952, 'b': 276.072, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 20.0]}]",
text,"You are able to call any of the following Python functions, if required, as often as you want:","[{'page_no': 10, 'bbox': {'l': 93.471, 't': 275.006, 'r': 368.541, 'b': 269.769, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 94.0]}]",
text,"1. execute_trajectory(position: list[float], orientation: float) -> None: This function will execute the trajectory on the robot arm end-effector","[{'page_no': 10, 'bbox': {'l': 105.206, 't': 268.702, 'r': 532.985, 'b': 263.4649999999999, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 145.0]}]",
text,"based on position and orientation, and will also not return anything. It takes list position of 3 elements and one float orientation value as input.","[{'page_no': 10, 'bbox': {'l': 81.833, 't': 262.398, 'r': 515.297, 'b': 257.16100000000006, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 148.0]}]",
text,"2. open_gripper() -> None: This function will open the gripper on the robot arm, and will also not return anything.","[{'page_no': 10, 'bbox': {'l': 105.206, 't': 256.09400000000005, 'r': 441.756, 'b': 250.85799999999995, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 115.0]}]",
list_item,"close_gripper(object_name: str) -> None: This function will close the gripper on the robot arm, and will also not return anything. It takes the name of the object as input.","[{'page_no': 10, 'bbox': {'l': 81.833, 't': 249.79099999999994, 'r': 524.111, 'b': 238.25, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 175.0]}]",
text,4. task_completed() -> None: Call this function only when the task has been completed. This function will also not return anything.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 237.183, 'r': 491.951, 'b': 231.94600000000003, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 131.0]}]",
section_header,CODE GENERATION:,"[{'page_no': 10, 'bbox': {'l': 93.471, 't': 224.57600000000002, 'r': 140.232, 'b': 219.33899999999994, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 16.0]}]",1.0
text,"When generating the code for the trajectory, do the following:","[{'page_no': 10, 'bbox': {'l': 93.471, 't': 218.27199999999993, 'r': 274.873, 'b': 213.03499999999997, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 62.0]}]",
text,1. Mark code clearly with the ```python and ``` tags.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 211.96799999999996, 'r': 260.261, 'b': 206.731, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 53.0]}]",
list_item,"When mentioning the functions, specify the required parameters and clearly define them in the same code block before passing it to code executor. For execute_trajectory, define the position and orientation lists prior to it and mention object name in close_gripper(object_name) from <CURRENT ENVIRONMENT STATE>.","[{'page_no': 10, 'bbox': {'l': 81.833, 't': 205.66499999999996, 'r': 512.391, 'b': 187.81999999999994, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 314.0]}]",
list_item,Orientation parameter will always be z orientation of object to grasp or destination to be place.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 186.75299999999993, 'r': 400.716, 'b': 181.51700000000005, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 100.0]}]",
text,4. *Generate the code all in one go for all the steps;*.,"[{'page_no': 10, 'bbox': {'l': 105.206, 't': 180.45000000000005, 'r': 271.981, 'b': 175.21299999999997, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 56.0]}]",
text,"Use the robot arm end effector to ""[INSERT TASK]"" in the environment.","[{'page_no': 10, 'bbox': {'l': 81.833, 't': 167.84199999999998, 'r': 283.676, 'b': 162.60500000000002, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 69.0]}]",
text,"The robot arm end-effector is currently positioned at [INSERT EE POSITION], with the orientation [INSERT EE ORIENTATION], and the gripper is open.","[{'page_no': 10, 'bbox': {'l': 81.833, 't': 161.539, 'r': 509.484, 'b': 156.30200000000002, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 146.0]}]",
text,"**Remember you can only grasp the objects from its *CENTER*. Not from any other position. So to grasp the object, end effector has to be lowered down to","[{'page_no': 10, 'bbox': {'l': 81.833, 't': 155.24900000000002, 'r': 529.942, 'b': 149.99299999999994, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 152.0]}]",
text,center of object**,"[{'page_no': 10, 'bbox': {'l': 81.833, 't': 148.90700000000004, 'r': 134.407, 'b': 143.66999999999996, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 18.0]}]",
text,The positions and orientations  of all objects in the environment as follows:,"[{'page_no': 10, 'bbox': {'l': 81.833, 't': 142.60300000000007, 'r': 307.022, 'b': 137.36599999999999, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 77.0]}]",
text,<CURRENT ENVIRONMENT STATE>:,"[{'page_no': 10, 'bbox': {'l': 81.833, 't': 136.29899999999998, 'r': 163.661, 'b': 131.063, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 28.0]}]",
text,"""[INSERT CURRENT STATE ENVIRONMENT]""","[{'page_no': 10, 'bbox': {'l': 81.833, 't': 129.99599999999998, 'r': 187.101, 'b': 124.75900000000001, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 36.0]}]",
section_header,B. RLBench Tasks,"[{'page_no': 11, 'bbox': {'l': 54.0, 't': 736.628, 'r': 129.198, 'b': 728.04, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 16.0]}]",1.0
text,"We experimented with nine tasks from RLBench, which are listed in Table II along with the task instructions and success criteria.","[{'page_no': 11, 'bbox': {'l': 54.0, 't': 721.355, 'r': 557.998, 'b': 700.848, 'coord_origin': 'BOTTOMLEFT'}, 'charspan': [0.0, 129.0]}]",
