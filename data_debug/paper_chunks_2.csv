chunk_id,text,start_char,end_char,word_count,section_heading,prov,arxiv_id
section_1,"Financial Report Chunking for E ff ective Retrieval Augmented Generation
Abstract. Chunking information is a key step in Retrieval Augmented Generation (RAG). Current research primarily centers on paragraphlevel chunking. This approach treats all texts as equal and neglects the information contained in the structure of documents. We propose an expanded approach to chunk documents by moving beyond mere paragraph-level chunking to chunk primary by structural element components of documents. Dissecting documents into these constituent elements creates a new way to chunk documents that yields the best chunk size without tuning. We introduce a novel framework that evaluates how chunking based on element types annotated by document understanding models contributes to the overall context and accuracy of the information retrieved. We also demonstrate how this approach impacts RAG assisted Question & Answer task performance. Our research includes a comprehensive analysis of various element types, their role in e ff ective information retrieval, and the impact they have on the quality of RAG outputs. Findings support that element type based chunking largely improve RAG results on financial reporting. Through this research, we are also able to answer how to uncover highly accurate RAG.",0,1295,191,Financial Report Chunking for E ff ective Retrieval Augmented Generation,"[PaperProv(page_no=1, bbox=PaperBbox(l=163.08, t=522.452, r=452.345, b=328.13, coord_origin='BOTTOMLEFT'), charspan=[0.0, 1222.0])]",2510.03230v1
section_2,"1 Introduction
Existing approaches for document understanding use a combination of methods from the computer vision and natural language processing domains to identify the di ff erent components in a document. In the rapidly evolving landscape of artificial intelligence, the capability to e ff ectively process unstructured data is becoming increasingly critical. Large Language Models (LLMs) like GPT-4 have revolutionized natural language understanding and generation, as evidenced by their prompt-based functionalities [31], enabling a wide range of applications [5]. However, the e ffi cacy of these models is often constrained by their reliance on the size and quality of the data they process. A notable limitation is the restricted contextual window of LLMs, which hampers their ability to fully comprehend the
contents of extensive documents [25,22,18]. By dissecting large volumes of text into smaller, more focused segments, LLMs can process each part with greater precision, ensuring a thorough understanding of each section. This segmented approach allows for meticulous analysis of unstructured data, enabling LLMs to construct a more comprehensive and coherent understanding of the entire document [41]. There remains a challenge in ensuring factual accuracy and relevance in the generated responses, especially when dealing with complex or extensive information.",0,1378,199,1 Introduction,"[PaperProv(page_no=1, bbox=PaperBbox(l=134.76, t=243.27600000000007, r=480.729, b=126.90899999999999, coord_origin='BOTTOMLEFT'), charspan=[0.0, 803.0]), PaperProv(page_no=2, bbox=PaperBbox(l=134.76, t=672.875, r=480.777, b=580.268, coord_origin='BOTTOMLEFT'), charspan=[0.0, 559.0])]",2510.03230v1
section_3,"1 Introduction
Recently, Retrieval Augmented Generation (RAG) [21,12] has been developed to address the hallucination problem with LLMs [15,43] when recovering factual information directly from an LLM. In RAG, instead of answering a user query directly using an LLM, the user query is used to retrieve documents or segments from a corpus and the top retrieved documents or segments are used to generate the answer in conjunction with an LLM. In this way, RAG constraints the answer to the set of retrieved documents. RAGs have been used as well to answer questions from single documents [14]. The documents are split into smaller parts or chunks, indexed by a retrieval system and recovered and processed depending on the user information need. In a sense, this process allows answering questions about information in a single document, thus contributing to the set of techniques available for document understanding.",0,917,145,1 Introduction,"[PaperProv(page_no=2, bbox=PaperBbox(l=134.76, t=576.515, r=480.901, b=436.148, coord_origin='BOTTOMLEFT'), charspan=[0.0, 902.0])]",2510.03230v1
section_4,"1 Introduction
Since documents need to be chunked for RAG processing, this raises the question about what is the best practice to chunk documents for e ff ective RAG document understanding. There are several dimensions to consider when deciding how to chunk a document, which includes the size of the chunks.
The retrieval system in RAG can use traditional retrieval systems using bagof-words methods or a vector database. If a vector database is used, then an embedding needs to be obtained from each chunk, thus the number of tokens in the chunk is relevant since the neural networks processing the chunks might have constraints on the number of tokens. As well, di ff erent chunk sizes might have undesirable retrieval results. Since the most relevant retrieved chunks need to be processed by an LLM, the number of tokens in retrieved chunks might have an e ff ect in the generation of the answer [25]. As we see, chunking is required for RAG systems and there are several advantages and disadvantages when considering how to chunk a document.",0,1046,177,1 Introduction,"[PaperProv(page_no=2, bbox=PaperBbox(l=134.76, t=432.395, r=480.611, b=387.668, coord_origin='BOTTOMLEFT'), charspan=[0.0, 293.0]), PaperProv(page_no=2, bbox=PaperBbox(l=134.76, t=383.915, r=480.802, b=267.428, coord_origin='BOTTOMLEFT'), charspan=[0.0, 737.0])]",2510.03230v1
section_5,"1 Introduction
In this work, we study specifically the chunking of U.S. Securities and Exchange Commission (SEC) 1 Financial Reports 2 , including 10-Ks, 10-Qs, and 8-Ks. This study plays a critical role in o ff ering insights into the financial health and operational dynamics of public companies. These documents present unique challenges in terms of document processing and information extraction as they consist of varying sizes and layouts, and contain a variety of tabular information. Previous work has evaluated the processing of these reports with simple chunking strategies (e.g., tokens), but we believe that a more e ff ective use of these reports might be achieved by a better pre-processing of the documents
and chunking configuration 3 [14]. To the best of our knowledge, this is the first systematic study on chunking for document understanding and more specifically for processing financial reports.",0,916,143,1 Introduction,"[PaperProv(page_no=2, bbox=PaperBbox(l=134.76, t=263.67499999999995, r=480.678, b=159.188, coord_origin='BOTTOMLEFT'), charspan=[0.0, 706.0]), PaperProv(page_no=3, bbox=PaperBbox(l=134.76, t=674.024, r=480.627, b=640.028, coord_origin='BOTTOMLEFT'), charspan=[0.0, 194.0])]",2510.03230v1
section_6,"2 Related work
RAG is an innovative method that has emerged to enhance the performance of LLMs by incorporating external knowledge, thereby boosting their capabilities. This technique has undergone substantial research, examining various configurations and applications. Key research includes Gao et al.'s [12] detailed analysis of RAG configurations and their role in enhancing Natural Language Processing (NLP) tasks, reducing errors, and improving factual accuracy. Several context retrieval methods are proposed to dynamically retrieve documents to improve the coherence of generated outputs [1]. Other research introduced advancements in RAG, including reasoning chain storage and optimization strategies for retrieval, respectively, broadening the scope and e ffi ciency of RAG applications in LLMs [21]. More recent work has compared RAG vs LLM fine-tuning, and identified that applying both improves the performance of each individual method [2].",0,954,133,2 Related work,"[PaperProv(page_no=3, bbox=PaperBbox(l=134.76, t=587.195, r=480.783, b=446.828, coord_origin='BOTTOMLEFT'), charspan=[0.0, 939.0])]",2510.03230v1
section_7,"2 Related work
Chunking has been identified as the key factor in the success of RAG, improving the relevance of retrieved content by ensuring accurate embedding of text with minimal noise. Various strategies have been developed for text subdivision, each with its unique approach. They can be summarized as follows: the fixed size strategy divides text into uniform segments, but it often overlooks the underlying textual structure. In contrast, the recursive strategy iteratively subdivides text using separators like punctuation marks, allowing it to adapt more fluidly to the content. The contextual strategy takes this a step further by employing NLP techniques such as sentence segmentation to represent the meaning in context. Lastly, the hybrid strategy combines di ff erent approaches, o ff ering greater flexibility in handling diverse text types [34]. However, an area yet to be explored in RAG chunking based on element types (document structure), which involves analyzing the inherent structure of documents, such as headings, paragraphs, tables, to guide the chunking process. Although chunking by Markdown and LaTeX comes closer to addressing element types, it's not the same in nature as a dedicated approach that directly considers document structure and element types for chunking, which could potentially yield more contextually relevant chunks.",0,1363,205,2 Related work,"[PaperProv(page_no=3, bbox=PaperBbox(l=134.76, t=443.195, r=480.845, b=231.06799999999998, coord_origin='BOTTOMLEFT'), charspan=[0.0, 1348.0])]",2510.03230v1
section_8,"2 Related work
Exploring the structure of financial reports is an exceptional area for establishing optimal principles for chunking. The intricate nature of document structures and contents has resulted in most of the work processing financial reports focusing on the identification of structural elements. Among previous work, we find El-Haj et al. [10] and the FinTOC challenges [17,4,11] that have worked at the document structure level for UK and French financial reports. Ad-
ditionally, there is recent work that considers U.S. SEC reports, which includes DocLayNet [33] and more specifically with the report tables in FinTabNet [45].
On the side of financial models, there is work in sentiment analysis in finance [37], which includes the pre-training of specialised models such as FinBERT by Liu et al. [26], which is a BERT based model pre-trained on large corpora including large collections of financial news collected from di ff erent sites and FinBERT by DeSola et al, [9] trained on Wikipedia, BookCorpus and U.S. SEC data. Additional models include BloombergGPT [40], FinGPT [42] and Instruct-FinGPT[44].",0,1119,173,2 Related work,"[PaperProv(page_no=3, bbox=PaperBbox(l=134.76, t=227.43399999999997, r=480.722, b=158.707, coord_origin='BOTTOMLEFT'), charspan=[0.0, 465.0]), PaperProv(page_no=4, bbox=PaperBbox(l=134.76, t=672.875, r=480.566, b=652.028, coord_origin='BOTTOMLEFT'), charspan=[0.0, 159.0]), PaperProv(page_no=4, bbox=PaperBbox(l=134.76, t=648.875, r=480.707, b=568.388, coord_origin='BOTTOMLEFT'), charspan=[0.0, 478.0])]",2510.03230v1
section_9,"2 Related work
More advance datasets in the financial domain include FinQA [6], LLMWare [27], ConFIRM [8] and TAT-QA [46] among others [7,38,19] that have been prepared for retrieval and or Questions and Answering (Q&A) tasks over snippets of financial data that includes tabular data, which has allowed methods on large language models to be tested on them [39].
Most of the previous work has focused on understanding the layout of financial documents or understanding specific snippets of existing reports with di ff erent levels of complexity, but there has not been much research in understanding financial report documents, except some more recent work that includes FinanceBench [14], in which a set of questions about the content of financial reports are proposed that includes the evidence snippet.",0,806,127,2 Related work,"[PaperProv(page_no=4, bbox=PaperBbox(l=134.76, t=565.235, r=494.28, b=508.628, coord_origin='BOTTOMLEFT'), charspan=[0.0, 348.0]), PaperProv(page_no=4, bbox=PaperBbox(l=134.76, t=505.475, r=480.678, b=436.868, coord_origin='BOTTOMLEFT'), charspan=[0.0, 442.0])]",2510.03230v1
section_10,"2 Related work
More specifically on document chunking methods for RAG, there are standard approaches being considered such as chunking text into spans of a given token length (e.g. 128 and 256) or chunking based on sentences. Open source projects already allow simple processing of documents (e.g. Unstructured 4 , Llamaindex 5 or Langchain 6 ), without explicitly considering the table structure on which these chunking strategies are applied.
Even though di ff erent approaches are available, an exhaustive evaluation of chunking applied to RAG and specifically to financial reporting, except for some limited chunking analysis [14,36], is non-existent. In our work, we compare a broad range of chunking approaches in addition to more simple ones and provide an analysis of the outcomes of di ff erent methods when asking questions about di ff erent aspects of the reports.",0,875,139,2 Related work,"[PaperProv(page_no=4, bbox=PaperBbox(l=134.759, t=433.715, r=480.73, b=365.108, coord_origin='BOTTOMLEFT'), charspan=[0.0, 429.0]), PaperProv(page_no=4, bbox=PaperBbox(l=134.759, t=361.955, r=480.703, b=293.348, coord_origin='BOTTOMLEFT'), charspan=[0.0, 430.0])]",2510.03230v1
section_11,"3 Methods
In this section, we present the chunking strategies that we have evaluated. Before describing the chunking strategies, we present the RAG environment in which these strategies have been evaluated and the dataset used for evaluation.",0,242,37,3 Methods,"[PaperProv(page_no=4, bbox=PaperBbox(l=134.759, t=253.59500000000003, r=480.703, b=220.74800000000005, coord_origin='BOTTOMLEFT'), charspan=[0.0, 232.0])]",2510.03230v1
section_12,"3.1 RAG setting for the experiments
The RAG pipeline used to process a user question is presented in figure 1 and is a common instance of a RAG [12]. Prior to answering any question about a given
document, the document is split into chunks and the chunks are indexed into a vector database (vectordb). When a question is sent to the RAG system, the top-k chunks most similar to the question are retrieved from the vector database and used to generate the answer using a large language model as generator. In order to retrieve chunks from the vector database, the question is encoded into a vector that is compared to the vector previously generated from the chunks. To prompt the generator, the question is converted into a set of instructions that instruct the LLM to find the answer within the top-k retrieved chunks.
In our experiments, we modify the way documents are chunked prior to being indexed in the vector database. All other settings remain constant. In the following sections, we describe in more detail each one of the components and processes used.",0,1063,183,3.1 RAG setting for the experiments,"[PaperProv(page_no=4, bbox=PaperBbox(l=134.759, t=186.995, r=480.73, b=166.14800000000002, coord_origin='BOTTOMLEFT'), charspan=[0.0, 159.0]), PaperProv(page_no=5, bbox=PaperBbox(l=134.76, t=672.875, r=480.699, b=580.268, coord_origin='BOTTOMLEFT'), charspan=[0.0, 623.0]), PaperProv(page_no=5, bbox=PaperBbox(l=134.759, t=433.475, r=480.601, b=388.748, coord_origin='BOTTOMLEFT'), charspan=[0.0, 243.0])]",2510.03230v1
section_13,"3.2 Indexing and retrieval
We have used the open source system Weaviate 7 as our vector database. As encoder model, we have used a sentence transformer [35] trained on over 256M questions and answers, which is available from the HuggingFace system 8 .
As shown in figure 2, to index a document, first the document is split into chunks, then each chunk is processed by an encoder model and then indexed into the vector database. Based on the chunking strategy a document will be split into a larger or smaller set of chunks.
As shown in figure 1, to retrieve chunks relevant to a question, the question is converted into a vector representation and the vector database returns a ranked list of chunks based on the similarity between question vector and the chunks in the database. Weaviate implements an approximate nearest neighbours algorithm [28] as their retrieval approach, which supports fast retrieval with high accuracy. In our experiments, we retrieve the top-10 chunks for each question.",0,996,166,3.2 Indexing and retrieval,"[PaperProv(page_no=5, bbox=PaperBbox(l=134.759, t=345.584, r=480.727, b=311.708, coord_origin='BOTTOMLEFT'), charspan=[0.0, 224.0]), PaperProv(page_no=5, bbox=PaperBbox(l=134.759, t=308.195, r=480.623, b=263.4680000000001, coord_origin='BOTTOMLEFT'), charspan=[0.0, 271.0]), PaperProv(page_no=6, bbox=PaperBbox(l=134.76, t=672.875, r=480.799, b=604.268, coord_origin='BOTTOMLEFT'), charspan=[0.0, 472.0])]",2510.03230v1
section_14,"3.3 Generation
Once the vector database has retrieved the top-10 chunks based on a question, the generation module generates the answer. To do so, a prompt based on the question and the retrieved chunks are provided to a large language model that generates the answer of the system.
We have used GPT-4 [31] as the generator, which has shown best performance compared to earlier versions. As well, its performance was better compared to existing open source alternatives [22] such as Mixtral [16]. We used the prompt presented in figure 3 that we designed on another similar RAG implementation with di ff erent document types. The prompt conditions the answer to the query and the chunks, referred to as source , and if the generator cannot answer it should return No answer .",0,775,132,3.3 Generation,"[PaperProv(page_no=6, bbox=PaperBbox(l=134.76, t=566.075, r=480.627, b=521.468, coord_origin='BOTTOMLEFT'), charspan=[0.0, 267.0]), PaperProv(page_no=6, bbox=PaperBbox(l=134.759, t=518.315, r=480.706, b=437.708, coord_origin='BOTTOMLEFT'), charspan=[0.0, 492.0])]",2510.03230v1
section_15,"3.4 Chunking
As a baseline chunking method, we have split the documents into chunks of size n tokens ( n ∈ ¶ 128 ↪ 256 ↪ 512 ♦ ). As well, an aggregation of the output by the indexing of di ff erent chunking configurations has been considered.
In addition to chunking based on the number of tokens, we have processed the documents using computer vision and natural language processing to extract elements identified in the reports. The list of elements considered are provided by the Unstructured 9 open source library. From the set of processing strategies,
we use Chipper, a vision encoder decoder 10 model inspired by Donut [20] to showcase the performance di ff erence. The Chipper model outputs results as a JSON representation of the document, listing elements per page characterized by their element type. Additionally, Chipper provides a bounding box enclosing each element on the page and the corresponding element text.",0,929,155,3.4 Chunking,"[PaperProv(page_no=6, bbox=PaperBbox(l=134.76, t=236.31500000000005, r=480.583, b=203.46799999999996, coord_origin='BOTTOMLEFT'), charspan=[0.0, 230.0]), PaperProv(page_no=6, bbox=PaperBbox(l=134.76, t=200.43499999999995, r=480.746, b=155.70799999999997, coord_origin='BOTTOMLEFT'), charspan=[0.0, 314.0]), PaperProv(page_no=7, bbox=PaperBbox(l=134.76, t=674.024, r=480.852, b=616.148, coord_origin='BOTTOMLEFT'), charspan=[0.0, 370.0])]",2510.03230v1
section_16,"3.4 Chunking
These elements are sometimes short to be considered as chunks, so to generate chunks from elements the following steps have been followed. Given the structure of finance reporting documents, our structural chunking e ff orts are concentrated on processing titles, texts, and tables. The steps to generate elementbased chunks are:
-if the element text length is smaller than 2,048 characters, a merge with the following element is attempted
-iteratively, element texts are merged following the step above till either the desired length is achieved, without breaking the element
-if a title element is found, a new chunk is started
-if a table element is found, a new chunk is started, preserving the entire table
After element-based chunks have been derived, three types of metadata are generated to enrich the content and support e ffi cient indexing. The first two types, generated via predefined prompt templates with GPT-4, include: 1) up to 6 representative keywords of the composite chunk 2) a summarised paragraph of the composite chunk. The third type is 3) Naive representation using the first two sentences from a composite chunk (a kind of prefix) and in the case of tables, the description of the table, which is typically identified in the table caption.",0,1279,207,3.4 Chunking,"[PaperProv(page_no=7, bbox=PaperBbox(l=134.76, t=613.115, r=480.679, b=556.388, coord_origin='BOTTOMLEFT'), charspan=[0.0, 329.0]), PaperProv(page_no=7, bbox=PaperBbox(l=141.0, t=544.475, r=480.581, b=523.628, coord_origin='BOTTOMLEFT'), charspan=[0.0, 109.0]), PaperProv(page_no=7, bbox=PaperBbox(l=141.0, t=520.715, r=480.589, b=499.868, coord_origin='BOTTOMLEFT'), charspan=[0.0, 136.0]), PaperProv(page_no=7, bbox=PaperBbox(l=141.0, t=496.955, r=366.132, b=488.108, coord_origin='BOTTOMLEFT'), charspan=[0.0, 52.0]), PaperProv(page_no=7, bbox=PaperBbox(l=141.0, t=485.195, r=480.582, b=476.348, coord_origin='BOTTOMLEFT'), charspan=[0.0, 81.0]), PaperProv(page_no=7, bbox=PaperBbox(l=134.76, t=464.195, r=480.843, b=383.708, coord_origin='BOTTOMLEFT'), charspan=[0.0, 554.0])]",2510.03230v1
section_17,"3.5 Dataset
To evaluate the performance of the di ff erent chunking approaches, we have used the FinanceBench dataset [14]. FinanceBench is a new benchmarking dataset designed to assess the capabilities of LLMs in answering open-book financial questions. The questions collected are realistic and applicable to real-world financial scenarios and include complex questions that require computational reasoning to arrive at conclusive answers.
This dataset is made of 150 instances with questions and answers from 84 unique reports. The dataset does not include the source documents, which we have downloaded. We were able to recover only 80 documents, which reduces the number of questions to 141 from the original 150. The distribution of Unstructured elements predictions are shown in table 1.",0,794,120,3.5 Dataset,"[PaperProv(page_no=7, bbox=PaperBbox(l=134.76, t=344.195, r=480.782, b=275.58799999999997, coord_origin='BOTTOMLEFT'), charspan=[0.0, 429.0]), PaperProv(page_no=7, bbox=PaperBbox(l=134.76, t=272.43499999999995, r=480.689, b=215.82799999999997, coord_origin='BOTTOMLEFT'), charspan=[0.0, 352.0])]",2510.03230v1
section_18,"3.5 Dataset
Documents have a varying number of pages, spanning from 4 pages (FOOTLOCKER 2022 8K dated-2022-05-20) to 549 pages (e.g. PEPSICO 2021 10K), with an average of 147.34 with std 97.78 with a total of 11,787 pages combined. Each instance contains a link to the report, the question, a question type , the answer and supporting evidence, with page number where the evidence is located
in the document, that allows for a closer evaluation of the results. Based on the page number, evidence contexts are located in di ff erent areas in the documents, ranging from the first page in some cases up to page 304 in one instance. The mean page number to find the evidence is 54.58 with a standard deviation of 43.66, which shows that evidence contexts to answer the questions are spread within a document. These characteristics make FinanceBench a perfect dataset for evaluating RAG. An example instance is available in table 2.",0,928,158,3.5 Dataset,"[PaperProv(page_no=7, bbox=PaperBbox(l=134.76, t=212.67499999999995, r=480.714, b=156.06799999999998, coord_origin='BOTTOMLEFT'), charspan=[0.0, 379.0]), PaperProv(page_no=8, bbox=PaperBbox(l=134.76, t=476.915, r=480.76, b=396.428, coord_origin='BOTTOMLEFT'), charspan=[0.0, 536.0])]",2510.03230v1
section_19,"4 Results
In this section, we evaluate the di ff erent chunking strategies using the FinanceBench dataset. Our evaluation is grounded in factual accuracy, which allows us to measure the e ff ectiveness of each configuration by its precision in retrieving answers that match the ground truth, as well as its generation abilities.
We are considering 80 documents and 141 questions from FinanceBench. Using the OpenAI tokenizer from the model text-embedding-ada-002 that uses the tokenizer cl100k base 11 , there are on average 102,444.35 tokens with std of 61,979.45, which shows the large variability of document lengths as seen by the di ff erent number of pages per document presented above.",0,692,111,4 Results,"[PaperProv(page_no=8, bbox=PaperBbox(l=134.76, t=344.555, r=491.651, b=299.828, coord_origin='BOTTOMLEFT'), charspan=[0.0, 318.0]), PaperProv(page_no=8, bbox=PaperBbox(l=134.76, t=296.195, r=480.716, b=239.58799999999997, coord_origin='BOTTOMLEFT'), charspan=[0.0, 363.0])]",2510.03230v1
section_20,"4 Results
Chunking E ffi ciency The first thing we analyzed is the total number of chunks, as it impacts indexing time. We would like to observe the relationship between accuracy and total chunk size. Table 3 shows the number of chunks derived from each one of the processing methods. Unstructured element-based chunks are closer in size to Base 512, and as the chunk size decreases for the basic chunking strategies, the total number of chunks increases linearly.
Retrieval Accuracy Secondly, we evaluate the capabilities of each chunking strategy in terms of retrieval accuracy. We use the page numbers in the ground truth to calculate the page-level retrieval accuracy, and we use ROUGE [24] and BLEU [32] scores to evaluate the accuracy of paragraph-level retrieval compared to the ground truth evidence paragraphs.",0,819,132,4 Results,"[PaperProv(page_no=8, bbox=PaperBbox(l=134.76, t=216.03499999999997, r=480.731, b=147.428, coord_origin='BOTTOMLEFT'), charspan=[0.0, 454.0]), PaperProv(page_no=9, bbox=PaperBbox(l=134.76, t=255.995, r=480.73, b=199.26800000000003, coord_origin='BOTTOMLEFT'), charspan=[0.0, 354.0])]",2510.03230v1
section_21,"4 Results
As shown in Table 4, when compared to Unstructured element-based chunking strategies, basic chunking strategies seem to have higher page-level retrieval accuracy but lower paragraph-level accuracy on average. Additionally, basic chunking strategies also lack consistency between page-level and paragraph-level accuracy; higher page-level accuracy doesn't ensure higher paragraph-level accuracy. For example, Base 128 has the second highest page-level accuracy but
A fascinating discovery is that when various chunking strategies are combined, it results in enhanced retrieval scores, achieving superior performance at both the page level (84.4%) and paragraph level (with ROUGE at 0.568% and BLEU at 0.452%). This finding addresses an unresolved question: how to improve the accuracy of RAG.",0,801,111,4 Results,"[PaperProv(page_no=9, bbox=PaperBbox(l=134.76, t=195.515, r=480.962, b=126.90800000000002, coord_origin='BOTTOMLEFT'), charspan=[0.0, 463.0]), PaperProv(page_no=10, bbox=PaperBbox(l=134.76, t=648.635, r=480.859, b=592.028, coord_origin='BOTTOMLEFT'), charspan=[0.0, 327.0])]",2510.03230v1
section_22,"4 Results
The element based method provides the highest scores and it also provides a mechanism to chunk documents without the need to fine tune hyper-parameters like the number of tokens in a chunk. This suggests the element based method is more generalizable and can be applied to new types of documents.
Q&A Accuracy Third, we evaluate the Q&A accuracy for the chunking strategies. In addition to manual evaluation, we have investigated an automatic evaluation using GPT-4. GPT-4 compares how the answers provided by our method are similar to or di ff erent from the FinanceBench gold standard, similar approaches have been previously evaluated [13,23,29,30]. The automatic evaluation allows scaling the evaluation e ff orts for the di ff erent chunking strategies that we have considered. We used the prompt template in figure 4.",0,833,134,4 Results,"[PaperProv(page_no=10, bbox=PaperBbox(l=134.76, t=588.635, r=480.927, b=543.908, coord_origin='BOTTOMLEFT'), charspan=[0.0, 296.0]), PaperProv(page_no=10, bbox=PaperBbox(l=134.76, t=521.675, r=480.806, b=441.068, coord_origin='BOTTOMLEFT'), charspan=[0.0, 526.0])]",2510.03230v1
section_23,"4 Results
Results in table 5 show that element-based chunking strategies o ff er the best question-answering accuracy, which is consistent with page retrieval and paragraph retrieval accuracy.
Lastly, our approach stands out for its e ffi ciency. Not only is element-based chunking generalizable without the need to select the chunk size, but when compared to the aggregation results that yield the highest retrieval scores. Elementbased chunking achieves the highest retrieval scores with only half the number of chunks required compared to methods that do not consider the structure of the documents (62,529 v.s. 112,155). This can reduce the indexing cost and improve query latency because there are only half as many vectors to index for the vectordb that stores the chunks. This underscores the e ff ectiveness of our solution in optimizing the balance between performance and computational resource requirements.",0,918,141,4 Results,"[PaperProv(page_no=10, bbox=PaperBbox(l=134.76, t=279.395, r=480.8, b=246.668, coord_origin='BOTTOMLEFT'), charspan=[0.0, 182.0]), PaperProv(page_no=10, bbox=PaperBbox(l=134.76, t=243.27499999999998, r=480.838, b=126.90800000000002, coord_origin='BOTTOMLEFT'), charspan=[0.0, 725.0])]",2510.03230v1
section_24,"5 Discussion
Results demonstrate the e ffi cacy of our approach in utilizing structural elements for chunking, which has enabled us to attain state-of-the-art performance on Q&A tasks within the FinanceBench dataset (accuracy of 50% vs 53.19%) when an index is created from document chunks and used for generation. This method, which we refer to as element base chunking , has shown to yield consistent results between retrieval and Q&A accuracy.
We have observed that using basic 512 chunking strategies produces results most similar to the Unstructured element-based approach, which may be due to the fact that 512 tokens share a similar length with the token size within our element-based chunks and capture a long context, but fail keep a coherent context in some cases, leaving out relevant information required for Q&A. This is further observed when considering the ROUGE and BLEU scores in table 4, where the chunk contexts for the baseline have lower scores.",0,966,156,5 Discussion,"[PaperProv(page_no=11, bbox=PaperBbox(l=134.76, t=318.275, r=480.687, b=249.668, coord_origin='BOTTOMLEFT'), charspan=[0.0, 433.0]), PaperProv(page_no=11, bbox=PaperBbox(l=134.76, t=244.95500000000004, r=480.749, b=164.34799999999996, coord_origin='BOTTOMLEFT'), charspan=[0.0, 519.0])]",2510.03230v1
section_25,"5 Discussion
These findings support existing research stating that the best basic chunk size varies from data to data [3]. These results show, as well, that our method adapts to di ff erent documents without tuning. Our method relies on the struc-
We have evaluated aggregating the output of di ff erent chunking methods in the retrieval experiments as sown in table 4. Even though the aggregation seems to be e ff ective for retrieval, the Q&A exceeded the GPT-4 token limit, which resulted in a non-e ff ective Q&A solution using the selected model.
As well, we evaluated variations of the prompt used to generate the answers (see figure 3). Re-ordering the retrieval context and the question, but results were not statistically di ff erent. We experimented as well with variations of the verbs using in the prompt, e.g. changing referencing with using , which seemed to lower the quality of the answers generated. This shows that prompt engineering is a relevant factor in RAG.
We evaluated using GPT-4 for evaluation instead of relying on manual evaluation. In most cases, GPT-4 evaluated correctly but failed when a more elaborate answer is provided. As shown in figure 5, the answer is 39.7% while the estimated answer is 39.73% but with a detailed explanation of the calculation.",0,1286,216,5 Discussion,"[PaperProv(page_no=11, bbox=PaperBbox(l=134.76, t=159.635, r=480.684, b=126.90800000000002, coord_origin='BOTTOMLEFT'), charspan=[0.0, 234.0]), PaperProv(page_no=12, bbox=PaperBbox(l=134.76, t=648.515, r=480.609, b=603.788, coord_origin='BOTTOMLEFT'), charspan=[0.0, 303.0]), PaperProv(page_no=12, bbox=PaperBbox(l=134.76, t=600.275, r=480.818, b=531.668, coord_origin='BOTTOMLEFT'), charspan=[0.0, 428.0]), PaperProv(page_no=12, bbox=PaperBbox(l=134.76, t=528.155, r=480.704, b=483.428, coord_origin='BOTTOMLEFT'), charspan=[0.0, 305.0])]",2510.03230v1
section_26,"6 Conclusions and Future Work
Results show that our element based chunking strategy improves the state-of-theart Q&A for the task, which is achieved by providing a better chunking strategy for the processed documents. We provide comparison with baseline chunking strategies that allow us to draw conclusions about di ff erent chunking methods.
As future work, we would like to perform a similar evaluation in other domains, e.g. biomedical, to understand how our findings apply outside financial reporting. As well, we would like studying which additional element types and/or relation between elements would support better chunking strategies for RAG.",0,652,98,6 Conclusions and Future Work,"[PaperProv(page_no=12, bbox=PaperBbox(l=134.76, t=219.755, r=480.691, b=175.14800000000002, coord_origin='BOTTOMLEFT'), charspan=[0.0, 313.0]), PaperProv(page_no=12, bbox=PaperBbox(l=134.76, t=171.635, r=480.797, b=126.90800000000002, coord_origin='BOTTOMLEFT'), charspan=[0.0, 308.0])]",2510.03230v1
table_1,"Table 1. Unstructured element types distribution for Chipper predictions against documents in FinanceBench.
| Element Type      | Chipper Entities   |
|:------------------|:-------------------|
| NarrativeText     | 61,780             |
| Title             | 29,664             |
| ListItem          | 33,054             |
| UncategorizedText | 9,400              |
| Footer            | 1,026              |
| Table             | 7,700              |
| Header            | 3,959              |
| Image             | 26                 |
| FigureCaption     | 54                 |
| Formula           | 29                 |
| Address           | 229                |
| Total             | 146,921            |",0,709,81,table,"[PaperProv(page_no=8, bbox=PaperBbox(l=226.549072265625, t=649.5025939941406, r=385.684326171875, b=503.4220886230469, coord_origin='BOTTOMLEFT'), charspan=[0.0, 0.0])]",2510.03230v1
table_2,"Table 2. Example question from the FinanceBench dataset
| Field                                 | Value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|:--------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| financebench id financebench id 00859 | financebench id financebench id 00859                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| doc name                              | VERIZON 2021 10K                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| doc link                              | https://www.verizon.com/about/sites/default/files/2021-Annual- Report-on-Form-10-K.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| question type                         | 'novel-generated'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| question                              | Among all of the derivative instruments that Verizon used to manage the exposure to fluctuations of foreign currencies exchange rates or interest rates, which one had the highest notional value in FY 2021?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| answer                                | Cross currency swaps. Its notional value was $ 32,502 million.,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| evidence text                         | Derivative Instruments We enter into derivative transactions primarily to manage our exposure to fluctuations in foreign currency exchange rates and interest rates. We employ risk management strategies, which may include the use of a variety of derivatives including interest rate swaps, cross currency swaps, forward starting interest rate swaps, trea- sury rate locks, interest rate caps, swaptions and foreign exchange for- wards. We do not hold derivatives for trading purposes. The following table sets forth the notional amounts of our outstanding derivative in- struments: (dollars in millions) At December 31, 2021 2020 Interest rate swaps $ 19,779 $ 17,768 Cross currency swaps 32,502 26,288 Forward starting interest rate swaps 1,000 2,000 Foreign exchange forwards 932 1,405 |
| page number                           | 85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |",0,8355,226,table,"[PaperProv(page_no=9, bbox=PaperBbox(l=133.40847778320312, t=649.3876953125, r=487.2038269042969, b=391.8861999511719, coord_origin='BOTTOMLEFT'), charspan=[0.0, 0.0])]",2510.03230v1
table_3,"Table 3. Chunks statistics for basic chunking elements and Unstructured elements
| Processing total chunks mean chunks per document (std) tables mean (std)   | Processing total chunks mean chunks per document (std) tables mean (std)   | Processing total chunks mean chunks per document (std) tables mean (std)   | Processing total chunks mean chunks per document (std) tables mean (std)   |
|:---------------------------------------------------------------------------|:---------------------------------------------------------------------------|:---------------------------------------------------------------------------|:---------------------------------------------------------------------------|
| Base 128                                                                   | 64,058                                                                     | 800.73 (484.11)                                                            | N/A                                                                        |
| Base 256                                                                   | 32,051                                                                     | 400.64 (242.04)                                                            | N/A                                                                        |
| Base 512                                                                   | 16,046                                                                     | 200.58 (121.01)                                                            | N/A                                                                        |
| Chipper                                                                    | 20,843                                                                     | 260.57 (145.80)                                                            | 96.20 (57.53)                                                              |",0,1940,105,table,"[PaperProv(page_no=9, bbox=PaperBbox(l=152.92388916015625, t=350.36297607421875, r=462.16912841796875, b=292.6167907714844, coord_origin='BOTTOMLEFT'), charspan=[0.0, 0.0])]",2510.03230v1
table_4,"Table 4. Retrieval results. For each chunking strategy, we show the number of chunks for all the documents (Total Chunks), Page Accuracy, and ROUGE and BLEU scores. ROUGE and BLEU are calculated as the maximum score from the list of recovered contexts for a question when compared to the known evidence for that question.
| Chunking strategy                  | Total Chunks   |   Page Accuracy |   ROUGE |   BLEU |
|:-----------------------------------|:---------------|----------------:|--------:|-------:|
| Base 128                           | 64,058         |           72.34 |   0.383 |  0.181 |
| Base 256                           | 32,051         |           73.05 |   0.433 |  0.231 |
| Base 512                           | 16,046         |           68.09 |   0.455 |  0.25  |
| Base Aggregation                   | 112,155        |           83.69 |   0.536 |  0.277 |
| Keywords Chipper                   | 20,843         |           46.1  |   0.444 |  0.315 |
| Summary Chipper                    | 20,843         |           62.41 |   0.473 |  0.35  |
| Prefix & Table Description Chipper | 20,843         |           67.38 |   0.514 |  0.4   |
| Chipper Aggregation                | 62,529         |           84.4  |   0.568 |  0.452 |",0,1251,168,table,"[PaperProv(page_no=11, bbox=PaperBbox(l=140.2692413330078, t=625.2447509765625, r=474.18865966796875, b=522.2947692871094, coord_origin='BOTTOMLEFT'), charspan=[0.0, 0.0])]",2510.03230v1
table_5,"Table 5. Q&A results. We show the percentage of questions with no answer and as well the accuracy either estimated automatically using GPT-4 or manually.
| Chunking strategy                  |   No answer |   GPT-4 |   Manual |
|:-----------------------------------|------------:|--------:|---------:|
| Base 128                           |       35.46 |   29.08 |    35.46 |
| Base 256                           |       25.53 |   32.62 |    36.88 |
| Base 512                           |       24.82 |   41.84 |    48.23 |
| Keywords Chipper                   |       22.7  |   43.97 |    53.19 |
| Summary Chipper                    |       17.73 |   43.97 |    51.77 |
| Prefix & Table Description Chipper |       20.57 |   41.13 |    53.19 |",0,745,100,table,"[PaperProv(page_no=11, bbox=PaperBbox(l=178.77072143554688, t=468.6895751953125, r=436.1873474121094, b=389.03924560546875, coord_origin='BOTTOMLEFT'), charspan=[0.0, 0.0])]",2510.03230v1
