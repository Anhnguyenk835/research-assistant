[
  {
    "chunk": {
      "chunk_metadata": {
        "start_char": 0,
        "chunk_word_count": 104,
        "section_heading": "III. METHOD",
        "end_char": 695,
        "prov": [
          {
            "bbox": {
              "r": 298.801,
              "b": 58.40700000000004,
              "t": 198.466,
              "coord_origin": "BOTTOMLEFT",
              "l": 54.0
            },
            "page_no": 3,
            "charspan": [
              0.0,
              683.0
            ]
          }
        ],
        "chunk_id": "section_9"
      },
      "arxiv_metadata": {
        "arxiv_id": "2510.08572v1",
        "pdf_url": ".data/arxiv_pdfs/2510.08572v1.pdf",
        "abstract": "Scaling data and models has played a pivotal role in the remarkable progress of computer vision and language. Inspired by these domains, recent efforts in robotics have similarly focused on scaling both data and model size to develop more generalizable and robust policies. However, unlike vision and language, robotics lacks access to internet-scale demonstrations across diverse robotic tasks and environments. As a result, the scale of existing datasets typically suffers from the need for manual data collection and curation. To address this problem, here we propose BLAZER, a framework that learns manipulation policies from automatically generated training data. We build on the zero-shot capabilities of LLM planners and automatically generate demonstrations for diverse manipulation tasks in simulation. Successful examples are then used to finetune an LLM and to improve its planning capabilities without human supervision. Notably, while BLAZER training requires access to the simulator's state, we demonstrate direct transfer of acquired skills to sensor-based manipulation. Through extensive experiments, we show BLAZER to significantly improve zero-shot manipulation in both simulated and real environments. Moreover, BLAZER improves on tasks outside of its training pool and enables downscaling of LLM models. Our code and data will be made publicly available on the project page.",
        "categories": [
          "cs.RO",
          "cs.AI",
          "cs.LG"
        ],
        "title": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data   Generation",
        "published_date": "2025-10-09T17:59:58Z",
        "authors": [
          "Rocktim Jyoti Das",
          "Harsh Singh",
          "Diana Turmakhan",
          "Muhammad Abdullah Sohail",
          "Mingfei Han",
          "Preslav Nakov",
          "Fabio Pizzati",
          "Ivan Laptev"
        ]
      },
      "chunk_text": "III. METHOD\nHere, we introduce the BLAZER methodology for training specialized LLM agents for robotic manipulation. In short, we aim to finetune an existing lightweight LLM on synthetically-generated data, tuning it for the generation of manipulation-oriented robotics policies. An overview of our method is shown in Fig. 2. This section first introduces the formalization and the problem setup (Section III-A), and we further describe how the BLAZER training works in Section III-B. Since our training procedure only exploits automatic annotations generated by a simulator, we also introduce a vision-based pipeline for the deployment in the real world of the trained LLM agent (Section III-C)."
    },
    "score": 6.784736,
    "chunk_id": "-Qd52JkBUTVOnGwZNBIi",
    "highlight": {
      "arxiv_metadata.abstract": [
        "To address this problem, here we propose BLAZER, a framework that learns manipulation policies from automatically <mark>generated</mark> training <mark>data</mark>."
      ],
      "arxiv_metadata.title": [
        "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot <mark>Data</mark>   <mark>Generation</mark>"
      ],
      "chunk_text": [
        "In short, we aim to finetune an existing lightweight LLM on synthetically-<mark>generated</mark> <mark>data</mark>, tuning it for the <mark>generation</mark> of manipulation-oriented robotics",
        "This section first introduces the formalization and the problem setup (Section III-A), and we further describe <mark>how</mark> the BLAZER training works in Section"
      ]
    }
  },
  {
    "chunk": {
      "chunk_metadata": {
        "start_char": 0,
        "chunk_word_count": 254,
        "section_heading": "C. Data Generation for Manipulation",
        "end_char": 1807,
        "prov": [
          {
            "bbox": {
              "r": 558.001,
              "b": 58.40700000000004,
              "t": 162.60000000000002,
              "coord_origin": "BOTTOMLEFT",
              "l": 313.2
            },
            "page_no": 2,
            "charspan": [
              0.0,
              516.0
            ]
          },
          {
            "bbox": {
              "r": 298.801,
              "b": 460.912,
              "t": 505.33,
              "coord_origin": "BOTTOMLEFT",
              "l": 54.0
            },
            "page_no": 3,
            "charspan": [
              0.0,
              183.0
            ]
          },
          {
            "bbox": {
              "r": 298.801,
              "b": 233.11400000000003,
              "t": 456.859,
              "coord_origin": "BOTTOMLEFT",
              "l": 54.0
            },
            "page_no": 3,
            "charspan": [
              0.0,
              1070.0
            ]
          }
        ],
        "chunk_id": "section_8"
      },
      "arxiv_metadata": {
        "arxiv_id": "2510.08572v1",
        "pdf_url": ".data/arxiv_pdfs/2510.08572v1.pdf",
        "abstract": "Scaling data and models has played a pivotal role in the remarkable progress of computer vision and language. Inspired by these domains, recent efforts in robotics have similarly focused on scaling both data and model size to develop more generalizable and robust policies. However, unlike vision and language, robotics lacks access to internet-scale demonstrations across diverse robotic tasks and environments. As a result, the scale of existing datasets typically suffers from the need for manual data collection and curation. To address this problem, here we propose BLAZER, a framework that learns manipulation policies from automatically generated training data. We build on the zero-shot capabilities of LLM planners and automatically generate demonstrations for diverse manipulation tasks in simulation. Successful examples are then used to finetune an LLM and to improve its planning capabilities without human supervision. Notably, while BLAZER training requires access to the simulator's state, we demonstrate direct transfer of acquired skills to sensor-based manipulation. Through extensive experiments, we show BLAZER to significantly improve zero-shot manipulation in both simulated and real environments. Moreover, BLAZER improves on tasks outside of its training pool and enables downscaling of LLM models. Our code and data will be made publicly available on the project page.",
        "categories": [
          "cs.RO",
          "cs.AI",
          "cs.LG"
        ],
        "title": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data   Generation",
        "published_date": "2025-10-09T17:59:58Z",
        "authors": [
          "Rocktim Jyoti Das",
          "Harsh Singh",
          "Diana Turmakhan",
          "Muhammad Abdullah Sohail",
          "Mingfei Han",
          "Preslav Nakov",
          "Fabio Pizzati",
          "Ivan Laptev"
        ]
      },
      "chunk_text": "C. Data Generation for Manipulation\nMany efforts have been dedicated to scale up training data for manipulation tasks. Some works collect real-world grasping data [31], [6] by deploying random trials followed by verification as well as collection of human demonstrations. KALIE [38] curated human-annotated affordance data as an alternative to robot demonstrations, whereas PhysObjects [39] collected an object-centric dataset to train a VLM with the physical concepts of common household objects. Another approach, LLaRA [40], adopted behavior cloning\ndatasets for instruction tuning tailored for manipulation tasks. This approach relies of external demonstrations and uses manually curated templates to generate question-answer pairs.\nAnother popular direction is to collect demonstrations in simulation. Nevertheless, this approach is still challenging as it requires significant human efforts to create diverse simulation tasks and scenes to allow generalization of the learned policies. To address this issue, recent work [8], [9] has leveraged coding LLMs with multimodal reasoning capabilities to generate complex and realistic simulation tasks without human supervision. DemoGen [41] adapted a single human trajectory to novel object configurations using 3D point cloud editing, yielding synthetic demonstrations that improve manipulation policies. Unlike prior work, our work focuses on scaling up demonstrations for a given set of tasks, while preserving generalization capabilities of LLMs. We propose a framework where we can generate an arbitrary number of data in simulation, benefiting from the reasoning and coding capabilities of LLMs to synthesize verification data with no human intervention. We subsequently use the generated data for the training of our agent tailored for manipulation."
    },
    "score": 4.630843,
    "chunk_id": "-Ad52JkBUTVOnGwZNBIi",
    "highlight": {
      "arxiv_metadata.abstract": [
        "To address this problem, here we propose BLAZER, a framework that learns manipulation policies from automatically <mark>generated</mark> training <mark>data</mark>."
      ],
      "arxiv_metadata.title": [
        "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot <mark>Data</mark>   <mark>Generation</mark>"
      ],
      "chunk_text": [
        "<mark>Data</mark> <mark>Generation</mark> for Manipulation\nMany efforts have been dedicated to scale up training <mark>data</mark> for manipulation tasks.",
        "We subsequently use the <mark>generated</mark> <mark>data</mark> for the training of our agent tailored for manipulation."
      ]
    }
  }
]